# logstash with grok
# https://www.elastic.co/guide/en/logstash/current/config-examples.html

$ cat /usr/share/logstash/d.conf                                                                                                                              
input { stdin { } }                                                                                                                                                                            
                                                                                                                                                                                               
filter {                                                                                                                                                                                       
  grok {                                                                                                                                                                                       
    match => { "message" => "%{COMBINEDAPACHELOG}" }                                                                                                                                           
  }                                                                                                                                                                                            
  date {                                                                                                                                                                                       
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]                                                                                                                                        
  }                                                                                                                                                                                            
}                                                                                                                                                                                              
                                                                                                                                                                                               
output {                                                                                                                                                                                       
  #elasticsearch { hosts => ["localhost:9200"] }                                                                                                                                               
  stdout { codec => rubydebug }                                                                                                                                                                
}                                                                                                                                                                                              
$ sudo /usr/share/logstash/bin/logstash -f /usr/share/logstash/d.conf                                                                                         
Using bundled JDK: /usr/share/logstash/jdk
......

127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"    ####Input from stdin

###parse results
{
          "http" => {
         "version" => "1.1",
         "request" => {
              "method" => "GET",
            "referrer" => "http://cadenza/xampp/navi.php"
        },
        "response" => {
            "status_code" => 200,
                   "body" => {
                "bytes" => 3891
            }
        }
    },
       "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv
:25.0) Gecko/20100101 Firefox/25.0\"",
     "timestamp" => "11/Dec/2013:00:01:45 -0800",
           "url" => {
        "original" => "/xampp/status.php"
    },
    "@timestamp" => 2013-12-11T08:01:45.000Z,
        "source" => {
        "address" => "127.0.0.1"
    },
          "host" => {
        "hostname" => "ha1"
    },
    "user_agent" => {
        "original" => "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
    },
      "@version" => "1",
         "event" => {
        "original" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; 
rv:25.0) Gecko/20100101 Firefox/25.0\""
    }
}

###https://www.elastic.co/blog/getting-started-with-the-elastic-stack-and-docker-compose
clean cache(danger!!!!)
 sudo rm -rf /data02/docker/volumes/docker-elk-release-6x_elasticsearch/_data
 
 backup and restore saved object(Visualize,Dashboard)
 using user elastic login-->management-->saved objects-->using import and export function
 
 
 
 https://kifarunix.com/backup-and-restore-elasticsearch-index-data/
 https://serverfault.com/questions/897183/how-to-take-elasticsearch-single-index-backup
 https://www.cyberithub.com/tutorial-elasticsearch-backup-and-restore/ 
 
 --0x1 prepare repo
$ cat elasticsearch/config/elasticsearch.yml 
---
## Default Elasticsearch configuration from Elasticsearch base image.
## https://github.com/elastic/elasticsearch/blob/6.8/distribution/docker/src/docker/config/elasticsearch.yml
#
cluster.name: "docker-cluster"
network.host: 0.0.0.0
path.repo: ["/elk-backup"]   ###key


$ docker-compose up -d
 
$ curl -u elastic:changeme -XPUT "http://192.168.56.109:9200/_snapshot/backup_repo" -H 'Content-Type: application/json' -d'
  {
        "type": "fs",
        "settings": {
           "location": "/elk_backup/",
           "compress": true
           }
  }'
   
$ alias curlx='curl -u elastic:changeme'
$ export ES=192.168.56.109:9200
$ curlx -XGET "$ES/_snapshot/backup_repo?pretty" 
 
$ curlx -XGET "$ES/_snapshot/_all?pretty"
 {
   "backup_repo" : {
     "type" : "fs",
     "settings" : {
       "compress" : "true",
       "location" : "/elk-backup/"
     }
   }
 }
 
 
 --0x2 backup
 $ curlx -XPUT "$ES/_snapshot/backup_repo/backup01?wait_for_completion=false" -H 'Content-Type: application/json' -d'  
 {
    "indices": "filebeat-6.8.23-2022.11.15,filebeat-6.8.23-2022.11.14", 
    "ignore_unavailable": true, 
    "include_global_state": true    
 }'                                                                                                                           
                                      
 {"accepted":true} ###respone,not input


 ##server repone 
 [2022-11-25T13:14:01,839][INFO ][o.e.s.SnapshotsService   ] [RkL6Od8] snapshot [backup_repo:backup01/MdOu2NFDTzuK5jgv9EU8NA] started
 [2022-11-25T13:14:03,057][INFO ][o.e.s.SnapshotsService   ] [RkL6Od8] snapshot [backup_repo:backup01/MdOu2NFDTzuK5jgv9EU8NA] completed with state [SUCCESS]
 
 --0x3 check backup
 [192 ]$  curlx -XGET "$ES"/_snapshot/backup_repo/backup01/?pretty                 
 {
   "snapshots" : [
     {
       "snapshot" : "backup01",
       "uuid" : "MdOu2NFDTzuK5jgv9EU8NA", 
       "version_id" : 6082399,
       "version" : "6.8.23",
       "indices" : [
         "filebeat-6.8.23-2022.11.15",
         "filebeat-6.8.23-2022.11.14"
       ],
       "include_global_state" : true,
       "state" : "SUCCESS",
       "start_time" : "2022-11-25T13:14:01.570Z",
       "start_time_in_millis" : 1669382041570,
       "end_time" : "2022-11-25T13:14:02.789Z",
       "end_time_in_millis" : 1669382042789,
       "duration_in_millis" : 1219,
       "failures" : [ ],
       "shards" : {
         "total" : 6,
         "failed" : 0,
         "successful" : 6
       }
     }
   ]
 }
 
 
 --0x4 delete index
 [192 ]$ curlx -X DELETE "$ES"/filebeat-6.8.23-2022.11.15
 {"acknowledged":true}
 
 [192 ]$ curlx -X DELETE "$ES"/filebeat-6.8.23-2022.11.14
 {"acknowledged":true}
 
 [2022-11-25T13:15:02,919][INFO ][o.e.c.m.MetaDataDeleteIndexService] [RkL6Od8] [filebeat-6.8.23-2022.11.15/Jy_o1c_EQJ-0QmmbEVdIpg] deleting index
 [2022-11-25T13:15:05,448][INFO ][o.e.c.m.MetaDataDeleteIndexService] [RkL6Od8] [filebeat-6.8.23-2022.11.14/478QGMB0Q6yxoQ1SlDCWOw] deleting index
 
 --0x5 restore index                                                                                                                                                                                                               
 [192 $ curlx -XPOST "$ES/_snapshot/backup_repo/snapshot_1/_restore?wait_for_completion=false" -H 'Content-Type: application/json' -d'
{
"indices": "filebeat-6.8.23-2022.11.15,filebeat-6.8.23-2022.11.14",
"ignore_unavailable": false
}'

###repone
 {
   "accepted" : true
 }
 
--0x5 from elk web portal 'Dev tool-->Console' operate
GET _cat/indices/_all?pretty

GET _cat/nodes

GET _cat/repositories?pretty

GET _snapshot/_all?pretty     

GET _snapshot/backup_repo/_all?pretty

DELETE _snapshot/backup_repo/filebeat_snopshot1?pretty

PUT _snapshot/backup_repo 
   {
         "type": "fs",
         "settings": {
            "location": "/elk-backup/",
            "compress": true
            }   
   }
   

POST _snapshot/backup_repo/backup01/_restore
{
"indices": "filebeat-6.8.23-2022.11.15,filebeat-6.8.23-2022.11.14",
"ignore_unavailable": false
}
===============================


https://mindmajix.com/elasticsearch/curl-syntax-with-examples
https://kifarunix.com/backup-and-restore-elasticsearch-index-data/
https://linuxhint.com/restore-elasticsearch-clusters-snapshots/
https://www.cyberithub.com/tutorial-elasticsearch-backup-and-restore/  ###**
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html
https://stackoverflow.com/questions/16290636/how-to-update-a-field-type-in-elasticsearch/16291163
https://onecompiler.com/posts/3tumbugg7/changing-type-of-an-existing-field-in-elasticsearch

https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html#snapshot-restore
https://qbox.io/blog/elasticsearch-data-snapshots-restore-tutorial

#####docker with custom password
https://www.elastic.co/guide/en/beats/heartbeat/current/configuring-ingest-node.html
https://stackoverflow.com/questions/40460830/filebeat-parse-fields-from-message-line
https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html
https://performanceengineeringsite.wordpress.com/2019/01/05/parsing-data-with-ingest-node-of-elastic-search/

https://www.elastic.co/guide/en/kibana/7.6/managing-indices.html
Close index. Blocks the index from read/write operations. A closed index exists in the cluster, but doesn’t consume resources other than disk space. If you reopen a closed index, it goes through the normal recovery process.
Force merge index. Reduces the number of segments in your shard by merging smaller files and clearing deleted ones. Only force merge a read-only index.
Refresh index. Writes the operations in the indexing buffer to the filesystem cache. This action is automatically performed once per second. Forcing a manual refresh is useful during testing, but should not be routinely done in production because it has a performance impact.
Clear index cache. Clears all caches associated with the index.
Flush index. Frees memory by syncing the filesystem cache to disk and clearing the cache. Once the sync is complete, the internal transaction log is reset.
Freeze index. Makes the index read-only and reduces its memory footprint by moving shards to disk. Frozen indices remain searchable, but queries take 


[mysql]
--10
# cat /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: false
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
output.elasticsearch:
  hosts: ["your_elk_srv_ip:9200"]
  indices:
    - index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"   ###create a index everyday
  protocol: "http"
  username: "elastic"
  password: "elastic_pwd"
setup.kibana:
  host: "your_elk_srv_ip:5601"
processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~

--20
# filebeat modules enable mysql

--30
# cat /etc/filebeat/modules.d/mysql.yml 
# Module: mysql
# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-module-mysql.html

- module: mysql
  # Error logs
  error:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:
    var.paths: ["/data/log/mysql/error.log"]
  # Slow logs
  slowlog:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:
    var.paths: ["/data/log/mysql-slow.log"]

--35
# filebeat setup
# systemctl start filebeat
# systemctl enable filebeat

--40
#tail -f /var/log/filebeat/filebeat
[mysql]

https://github.com/deviantony/docker-elk
# cat docker-compose.yml
version: '3.2'

services:
  elasticsearch:
    build:
      context: elasticsearch/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./elasticsearch/config/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: true
      - type: volume
        source: elasticsearch
        target: /usr/share/elasticsearch/data
    ports:
      - "192.168.2.88:9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx4096m -Xms4096m"
      ELASTIC_PASSWORD: Custom_password
    networks:
      - elk

#  logstash:
#    build:
#      context: logstash/
#      args:
#        ELK_VERSION: $ELK_VERSION
#    volumes:
#      - type: bind
#        source: ./logstash/config/logstash.yml
#        target: /usr/share/logstash/config/logstash.yml
#        read_only: true
#      - type: bind
#        source: ./logstash/pipeline
#        target: /usr/share/logstash/pipeline
#        read_only: true
#    ports:
#      - "5000:5000"
#      - "9600:9600"
#    environment:
#      LS_JAVA_OPTS: "-Xmx4096m -Xms4096m"
#    networks:
#      - elk
#    depends_on:
#      - elasticsearch

  kibana:
    build:
      context: kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./kibana/config/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
    environment:
        ELASTICSEARCH_URL: http://192.168.2.88:9200
        ELASTICSEARCH_USERNAME: elastic
        ELASTICSEARCH_PASSWORD: Custom_password
    ports:
      - "192.168.2.88:5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch

networks:
  elk:
    driver: bridge

volumes:
  elasticsearch:
#####docker with custom password



####basic license
https://www.elastic.co/guide/en/elasticsearch/reference/current/start-basic.html
POST /_license/start_basic
####basic license

###elk docker-compose
https://github.com/deviantony/docker-elk
###elk docker-compose

####linux rpm system log###
$curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.4.1-x86_64.rpm
$sudo rpm -vi filebeat-7.4.1-x86_64.rpm

$vim /etc/filebeat/filebeat.yml
# cat /etc/filebeat/filebeat.yml 
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:
  host: "example.com:5601"
output.elasticsearch:
  hosts: ["elasticsearch.example.com:9200"]
  protocol: "https"
  username: "elastic"
  password: "password_here"
processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~

$filebeat modules enable system


$sudo filebeat setup
$sudo service filebeat start

####linux rpm system log###


###nginx load metric####

10 nginx vm
$>wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.3.1-x86_64.rpm
$>rpm -ivh etricbeat-7.3.1-x86_64.rpm

$>vim /etc/metricbeat.yml
output.elasticsearch:
  hosts: ["<es_url>"]
  username: "elastic"
  password: "<password>"
setup.kibana:
  host: "<kibana_url>"

$ metricbeat modules enable nginx


$>vim /etc/metricbeat/modules.d/nginx.yml
# Module: nginx
# Docs: https://www.elastic.co/guide/en/beats/metricbeat/7.3/metricbeat-module-nginx.html

- module: nginx
  #metricsets:
  #  - stubstatus
  period: 10s

  # Nginx hosts
  hosts: ["http://127.0.0.1"]

  # Path to server status. Default server-status
  #server_status_path: "server-status"
  server_status_path: "basic_status"

  #username: "user"
  #password: "secret"

$>vim nginx.conf
        location = /basic_status {                                                                                                                                                                             
                    stub_status;
        }   

$>nginx -s reload

$>metricbeat setup

$>systemctl start metricbeat

$>systemctl status metricbeat -l

20 view on kinaba
http://192.168.2.8:5601-->home/add data/nginx metrics-->right down-->nginx metrics dashboard
###nginx load metric####



###########winlog event ###########
05 > windows 10 18362 version
10 >windows install

PS C:\Windows\system32> Set-ExecutionPolicy -ExecutionPolicy Unrestricted

PS C:\Windows\system32> d:

PS D:\> dir


    目录: D:\


Mode                LastWriteTime         Length Name                                                                                                                   
----                -------------         ------ ----                                                                                                                   
d-----       2019/10/18     15:06                winlogbeat                                                                                                             
-a----       2019/10/18     14:23       16084730 winlogbeat.zip                                                                                                         



PS D:\> cd .\winlogbeat

PS D:\winlogbeat> .\install-service-winlogbeat.ps1
##url        https://www.elastic.co/guide/en/beats/winlogbeat/current/elasticsearch-output.html
#secure      https://www.elastic.co/guide/en/beats/winlogbeat/current/securing-communication-elasticsearch.html
#https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication
Status   Name               DisplayName                           
------   ----               -----------                           
Stopped  winlogbeat         winlogbeat                            

d:\>vim winlogbeat.yml
output.elasticsearch:

winlogbeat.event_logs:
    - name: Application
      provider:
          - Application Error
          - Application Hang
          - Windows Error Reporting
          - EMET
    - name: Security
      level: critical, error, warning
      event_id: 4624, 4625, 4700-4800, -4735
    - name: System
      ignore_older: 168h
    - name: Microsoft-Windows-Windows Defender/Operational
      include_xml: true
output.elasticsearch:
    hosts: "elk-srv-ip-or-DNS"         ###port is 9200,not present here
    protocol: "https"                  ###https default port is 9200,interactive with nginx need PATT
    username: "elastic"
    password: "changeme"

PS D:\winlogbeat> Start-Service winlogbeat



PS D:\winlogbeat> Get-Service winlogbeat

Status   Name               DisplayName                           
------   ----               -----------                           
Running  winlogbeat         winlogbeat 


===================
20> portal view
http://192.168.2.88:5601/app/siem#



####with nginx
https://www.netways.de/blog/2017/09/14/secure-elasticsearch-and-kibana-with-an-nginx-http-proxy/
https://sysadmins.co.za/secure-your-elasticsearch-cluster-with-basic-auth-using-nginx-and-ssl-from-letsencrypt/

# cat elasticsearchabccn.conf
upstream elasticsearch {
    server 192.168.2.8:9200;
    keepalive 15;
}

server {
  listen 80;
  server_name elasticsearch.example.com
  return 301 https://elasticsearch.example.com:9200$request_uri;
}

server {
  listen 9200 ssl;                         ### LISTEN 9200 ####
  server_name elasticsearch.example.com
  ssl_session_timeout  10m;
  ssl_ciphers HIGH:!aNULL:!MD5;
  ssl_prefer_server_ciphers on;

  ssl_certificate /etc/letsencrypt/archive/example.com/fullchain1.pem;
  ssl_certificate_key /etc/letsencrypt/archive/example.com/privkey1.pem;

  #auth_basic "server auth";
  #auth_basic_user_file /etc/nginx/passwords;

  location ^~ /.well-known/acme-challenge/ {
    auth_basic off;
  }

  location / {

    # deny node shutdown api
    if ($request_filename ~ "_shutdown") {
      return 403;
      break;
    }

    proxy_pass http://elasticsearch;
    proxy_http_version 1.1;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $http_host;
    proxy_redirect off;
  }

  location = / {
    proxy_pass http://elasticsearch;
    proxy_http_version 1.1;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $http_host;
    proxy_redirect off;
    auth_basic "off";
  }

  location ~* ^(/_cluster/health|/_cat/health) {
    proxy_pass http://elasticsearch;
    proxy_http_version 1.1;
    proxy_set_header Connection "Keep-Alive";
    proxy_set_header Proxy-Connection "Keep-Alive";
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $http_host;
    proxy_redirect off;
    auth_basic "off";
  }
}

####with nginx
###########winlog event ###########
