#######install with ali ############
==================docker images ver
# docker images
REPOSITORY                                                             TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy                     v1.16.2             8454cbe08dc9        3 weeks ago         86.1MB
registry.aliyuncs.com/google_containers/kube-apiserver                 v1.16.2             c2c9a0406787        3 weeks ago         217MB
registry.aliyuncs.com/google_containers/kube-controller-manager        v1.16.2             6e4bffa46d70        3 weeks ago         163MB
registry.aliyuncs.com/google_containers/kube-scheduler                 v1.16.2             ebac1ae204a2        3 weeks ago         87.3MB
quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller   0.26.1              29024c9c6e70        6 weeks ago         483MB
quay.io/kubernetes-ingress-controller/nginx-ingress-controller         0.26.1              29024c9c6e70        6 weeks ago         483MB
registry.aliyuncs.com/google_containers/etcd    



###GFW
https://github.com/Azure/container-service-for-azure-china/blob/master/aks/README.md#limitations-of-current-aks-private-preview-on-azure-china


=========================
useful cmd
echo y|kubeadm reset && rm -rfv $HOME/.kube/config && ip link delete flannel.1                                                                 
kubeadm init --config=kubeadm-config.yaml --upload-certs

kubectl describe pod coredns-58cc8c89f4-jjrz8 -n kube-system

kubeadm init --image-repository=gcr.azk8s.cn/google_containers --pod-network-cidr=10.244.0.0/16 --upload-certs

kubectl apply -f kube-flannel.yaml  ######only run on first master

kubectl get pods -A -o wide

kubectl get nodes -A

kubectl logs -f etcd-k8s-m1  -n kube-system


kubectl get svc -o yaml


# ip a ####flannel.1 and cni0 MUST EXIST ON kubelet nodes and master
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:82:cb:d3:29 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 2a:ce:5f:d4:1e:91 brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::28ce:5fff:fed4:1e91/64 scope link
       valid_lft forever preferred_lft forever
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether da:4c:87:d2:0e:bb brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.1/24 scope global cni0
       valid_lft forever preferred_lft forever

=========================
https://yq.aliyun.com/articles/702158

--5 master


systemctl stop firewalld && systemctl disable firewalld
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab

cat > /etc/sysconfig/modules/ipvs.modules <<EOF
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

cat <<EOF >
 /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
yum install -y yum-utils device-mapper-persistent-data lvm2
yum -y install wget
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast
yum install -y docker-ce
systemctl start docker
systemctl enable docker


cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl start kubelet
systemctl enable --now kubelet
systemctl status kubelet


mkdir /etc/docker
# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
},
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
{
  “registry-mirrors”: [“https://registry.docker-cn.com“]
}
EOF

master $> cat kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
controlPlaneEndpoint: "10.0.2.22:6443"
imageRepository: "registry.aliyuncs.com/google_containers"
networking:
  podSubnet: "10.249.0.0/16"
apiServer:
  certSANs:
  - "k8s249.example.cn"





master $> kubeadm init --config=kubeadm-config.yaml --upload-certs




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
    --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e

# kubectl get pods -A
# The connection to the server localhost:8080 was refused - did you specify the right host or port?
master$>    export KUBECONFIG=/etc/kubernetes/admin.conf

or

maser$>    mkdir -p $HOME/.kube
maser$>    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
maser$>    sudo chown $(id -u):$(id -g) $HOME/.kube/config



--10 master another way
# hostnamectl set-hostname master


# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.33.10   master
192.168.33.11   node1


# cat preENV.sh
#!/bin/bash
# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld
# 关闭SELINUX
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
# 关闭Swap
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab

# sh preENV.sh

cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

maser$>    modprobe br_netfilter
maser$>    sysctl -p /etc/sysctl.d/k8s.conf


maser$>    cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
maser$>    chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4


maser$>    yum install -y yum-utils device-mapper-persistent-data lvm2
maser$>    wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
maser$>    yum makecache fast
maser$>    yum install -y docker-ce
maser$>    systemctl start docker
maser$>    systemctl enable docker


maser$>    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
maser$>    yum makecache fast
maser$>    yum install -y kubelet kubeadm kubectl
maser$>    systemctl enable kubelet



maser$>    vim k8sMasterImages.sh
#!/bin/bash

set -e

KUBE_VERSION=v1.14.1
KUBE_PAUSE_VERSION=3.1
ETCD_VERSION=3.3.10
CORE_DNS_VERSION=1.3.1

GCR_URL=k8s.gcr.io
ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers

images=(kube-proxy:${KUBE_VERSION}
kube-scheduler:${KUBE_VERSION}
kube-controller-manager:${KUBE_VERSION}
kube-apiserver:${KUBE_VERSION}
pause:${KUBE_PAUSE_VERSION}
etcd:${ETCD_VERSION}
coredns:${CORE_DNS_VERSION})


for imageName in ${images[@]} ; do
  docker pull $ALIYUN_URL/$imageName
  docker tag  $ALIYUN_URL/$imageName $GCR_URL/$imageName
  docker rmi $ALIYUN_URL/$imageName
done
maser$>    chmod +x k8sMasterImages.sh
maser$>    ./k8sMasterImages.sh



maser$>    kubeadm init --kubernetes-version=v1.14.1 --apiserver-advertise-address=192.168.33.10 --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.14.1
[preflight] Running pre-flight checks
    [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.33.10 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.33.10 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.33.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.502789 seconds
[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --experimental-upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 6ftxi9.gov5rsp9syw1fect
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
    --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e



maser$>    mkdir -p $HOME/.kube
maser$>    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
maser$>    sudo chown $(id -u):$(id -g) $HOME/.kube/config



maser$>    kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}


maser$>    kubectl get pod -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-fb8b8dccf-88hfr          0/1     Pending   0          6m33s   <none>          <none>   <none>           <none>
kube-system   coredns-fb8b8dccf-jmw7m          0/1     Pending   0          6m33s   <none>          <none>   <none>           <none>
kube-system   etcd-master                      1/1     Running   0          5m42s   192.168.33.10   master   <none>           <none>
kube-system   kube-apiserver-master            1/1     Running   0          5m46s   192.168.33.10   master   <none>           <none>
kube-system   kube-controller-manager-master   1/1     Running   0          5m47s   192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-46wth                 1/1     Running   0          6m32s   192.168.33.10   master   <none>           <none>
kube-system   kube-scheduler-master            1/1     Running   0          5m40s   192.168.33.10   master   <none>           <none>





maser$>    kubectl describe node master | grep Taint
Taints:             node-role.kubernetes.io/master:NoSchedule
maser$>    kubectl taint nodes master node-role.kubernetes.io/master-
node "master" untainted


maser$>    kubectl get nodes
NAME     STATUS     ROLES    AGE   VERSION
master   Ready      master   58m   v1.14.1
node1    NotReady   <none>   13s   v1.14.1

maser$>    watch kubectl get nodes
Every 2.0s: kubectl get node                                                                                                                            Sun May 12 02:48:00 2019

NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   12h   v1.14.1
node1    Ready    <none>   11h   v1.14.1

maser$>    cat processFlannelImage.sh
#!/bin/bash
docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64
maser$>    chmod +x processFlannelImage.sh
maser$>    ./processFlannelImage.sh


maser$>    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  ######only run on first master
podsecuritypolicy.extensions/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created


maser$>    kubectl get pod -A -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE    IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-fb8b8dccf-88hfr            1/1     Running   0          12h    10.244.0.3      master   <none>           <none>
kube-system   coredns-fb8b8dccf-jmw7m            1/1     Running   0          12h    10.244.0.2      master   <none>           <none>
kube-system   etcd-master                        1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-apiserver-master              1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-controller-manager-master     1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5jtfd        1/1     Running   0          11h    192.168.33.11   node1    <none>           <none>
kube-system   kube-flannel-ds-amd64-bgv9j        1/1     Running   0          11h    192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-46wth                   1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-r86jk                   1/1     Running   0          11h    192.168.33.11   node1    <none>           <none>
kube-system   kube-scheduler-master              1/1     Running   0          12h    192.168.33.10   master   <none>           <none>


cat /etc/haproxy/haproxy.cfg 
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend k8s-api
  bind *:6443
  mode tcp
  option tcplog
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-m01 192.168.200.212:6443 check
  server k8s-m02 192.168.200.213:6443 check


 cat kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "192.168.200.214:6443"
networking:
  podSubnet: "10.244.0.0/16"

#######install with ali ############
--20 node


systemctl stop firewalld && systemctl disable firewalld
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
vim /etc/sysctl.d/k8s.conf
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
cat > /etc/sysconfig/modules/ipvs.modules <<EOFmodprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
yum install -y yum-utils device-mapper-persistent-data lvm2
yum -y install wget
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast
yum install -y docker-ce
systemctl start docker
systemctl enable docker
systemctl status kubectl
systemctl enable kubectl
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF


## Create /etc/docker directory.
mkdir /etc/docker
# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
"exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"storage-driver": "overlay2",
"storage-opts": [
"overlay2.override_kernel_check=true"
]
}
{
“registry-mirrors”: [“https://registry.docker-cn.com“]
}
EOF

yum makecache fast
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet
systemctl status kubectl
kubeadm join 10.0.2.22:6443 --token kze0e4.x0jcxedjndu215y6     --discovery-token-ca-cert-hash sha256:a718d1ea16be1045898ef5926931f447189969888a98938502a5d4c54e5884c2



--20 another way
[root@node1 ~]$ cat k8sNodeImages.sh
#!/bin/bash

set -e

KUBE_VERSION=v1.14.1
KUBE_PAUSE_VERSION=3.1

GCR_URL=k8s.gcr.io
ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers

images=(kube-proxy-amd64:${KUBE_VERSION}
pause:${KUBE_PAUSE_VERSION})


for imageName in ${images[@]} ; do
  docker pull $ALIYUN_URL/$imageName
  docker tag  $ALIYUN_URL/$imageName $GCR_URL/$imageName
  docker rmi $ALIYUN_URL/$imageName
done
node$>    chmod +x k8sNodeImages.sh
node$>    ./k8sNodeImages.sh



node$>    kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
>     --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e
[preflight] Running pre-flight checks
    [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
node$>   




40 helm
master> tar vzxf helm-v2.16.0-linux-amd64.tar.gz
master> cd linux-amd64 && cp helm /bin && cp tiller /bin
master> kubectl create serviceaccount --namespace kube-system tiller

master> kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller


master> helm init --upgrade --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

master> kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'

master> kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount

master> kubectl -n kube-system get pods|grep tiller
master> helm repo add stable  http://mirror.azure.cn/kubernetes/charts

master>  kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount
      serviceAccount: tiller
      serviceAccountName: tiller

master>  kubectl -n kube-system get pods|grep tiller
tiller-deploy-6b5ffb6f-r4rpb     1/1     Running   0          15s

master>   helm repo add stable  http://mirror.azure.cn/kubernetes/charts
"stable" has been added to your repositories

master>   helm repo list
NAME    URL                                     
stable  http://mirror.azure.cn/kubernetes/charts
local   http://127.0.0.1:8879/charts           

master>  helm list                                                                                                                                                                   
####nothing  here is correct###

master>  helm version                                                                                                                                                                
Client: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666270cfbe734aaabf342a49bb", GitTreeState:"clean"}                                                                       
Server: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666270cfbe734aaabf342a49bb", GitTreeState:"clean"}                                                                       
master>  helm search mysql                                                                                                                                                           
NAME                                    CHART VERSION   APP VERSION     DESCRIPTION                                                                                                          
stable/mysql                            1.4.0           5.7.27          Fast, reliable, scalable, and easy to use open-source rel...                                                         
stable/mariadb                          7.0.1           10.3.20         Fast, reliable, scalable, and easy to use open-source rel...                                                         
master>  helm install stable/mysql                                                                                                                                                   
NAME:   bald-chimp                                                                                                                                                                           
LAST DEPLOYED: Tue Nov 12 03:45:16 2019                                                                                                                                                      
NAMESPACE: default                                                                                                                                                                           
STATUS: DEPLOYED                                                                                                                                                                             
RESOURCES:                                                                                                                                                                                   
==> v1/ConfigMap                                                                                                                                                                             
NAME                   AGE                                                                                                                                                                   
bald-chimp-mysql-test  0s                                                                                                                                                                    
==> v1/Deployment                                                                                                                                                                            
NAME              AGE                                                                                                                                                                        
bald-chimp-mysql  0s                                                                                                                                                                         
==> v1/PersistentVolumeClaim                                                                                                                                                                 
NAME              AGE                                                                                                                                                                        
bald-chimp-mysql  0s                                                                                                                                                                         
==> v1/Pod(related)                                                                                                                                                                          
NAME                               AGE                                                                                                                                                       
bald-chimp-mysql-6657c87fbc-994t5  0s    
                                                                                                                                                                                                                                     [3/4651]
==> v1/PersistentVolumeClaim
NAME              AGE
bald-chimp-mysql  0s

==> v1/Pod(related)
NAME                               AGE
bald-chimp-mysql-6657c87fbc-994t5  0s

==> v1/Secret
NAME              AGE
bald-chimp-mysql  0s

==> v1/Service
NAME              AGE
bald-chimp-mysql  0s

NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
bald-chimp-mysql.default.svc.cluster.local

To get your root password run:

    MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default bald-chimp-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)                                                                                        

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h bald-chimp-mysql -p

To connect to your database directly from outside the K8s cluster:
    MYSQL_HOST=127.0.0.1
    MYSQL_PORT=3306

    # Execute the following command to route the connection:
    kubectl port-forward svc/bald-chimp-mysql 3306

    mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}


master>  helm list        ####Installed chart appear                                                                                                                                                           
NAME            REVISION        UPDATED                         STATUS          CHART           APP VERSION     NAMESPACE                                                                    
singed-peacock  1               Tue Nov 12 03:43:17 2019        DEPLOYED        mysql-1.4.0     5.7.27          default                                                                      

master>  kubectl get svc
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
bald-chimp-mysql       ClusterIP   10.99.0.194      <none>        3306/TCP   11s
kubernetes             ClusterIP   10.96.0.1        <none>        443/TCP    20m
singed-peacock-mysql   ClusterIP   10.101.253.119   <none>        3306/TCP   2m10s

master>  nc -vz 10.96.0.1 443
Ncat: Version 7.50 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.1:443.
Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.

=====nginx-ingress
master> helm install stable/nginx-ingress --set defaultBackend.image.repository=gcr.azk8s.cn/google_containers/defaultbackend --set defaultBackend.image.tag=1.4

GFW DBG
kubectl get pods -A -o wide|grep defa

kubectl describe pod mewing-flee-nginx-ingress-controller-7db757cf94-fq2m6 -n default

kubectl logs kindly-goat-nginx-ingress-controller-bf99b859-ng4cz -n default

docker images
docker tag quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1

