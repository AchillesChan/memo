https://msazure.club/flannel-networking-demystify/
https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.laputa.io%2Fkubernetes-flannel-networking-6a1cb1f8ec7c
https://mvallim.github.io/kubernetes-under-the-hood/documentation/kube-flannel.html
https://github.com/sighupio/permission-manager   ####create rbac user on kubernetes cluster
https://github.com/kubernetes-sigs/prometheus-adapter/issues/164  ###unable to fetch metrics from custom metrics API
https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
https://github.com/dgkanatsios/CKAD-exercises#contents
https://rudimartinsen.com/2020/12/30/backup-restore-etcd/
https://github.com/larkintuckerllc/k8s-cka-tutorial
https://dev.to/subodev/how-to-prepare-for-the-ckad-exam-4oa3
https://github.com/bmuschko/ckad-prep#demos
https://github.com/bbachi/CKAD-Practice-Questions#table-of-contents
https://killer.sh/              #### CKAD/CKA/CKS exam simulator
https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad
https://github.com/walidshaari/Kubernetes-Certified-Administrator
https://istio.io/latest/docs/setup/install/helm/
https://www.padok.fr/en/blog/traefik-kubernetes-certmanager
https://kompose.io/    ###convert docker-compose.yaml to kubernete.yaml
https://doc.traefik.io/traefik/v1.7/user-guide/kubernetes/
https://www.padok.fr/en/blog/traefik-kubernetes-certmanager
https://doc.traefik.io/traefik/providers/kubernetes-ingress/
https://medium.com/@geraldcroes/kubernetes-traefik-101-when-simplicity-matters-957eeede2cf8
https://github.com/traefik/traefik
https://www.squadcast.com/blog/infrastructure-monitoring-using-kube-prometheus-operator
https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
https://blog.pvincent.io/2017/12/prometheus-blog-series-part-5-alerting-rules/
https://github.com/prometheus-community/helm-charts                ####prometheus and grafana
https://istio.io/latest/docs/ops/deployment/architecture/
https://istio.io/latest/docs/concepts/traffic-management/
https://www.cuelogic.com/blog/istio-service-mesh
https://istio.io/latest/docs/
https://www.servicemesher.com/istio-handbook/
https://banzaicloud.com/blog/logging-operator-monitoring/        #fluentbit
https://www.yld.io/blog/service-meshin-with-istio/
https://dzone.com/articles/metadata-management-in-big-data-systems-a-complete-1
https://dzone.com/articles/istio-service-mesh-the-step-by-step-guide-part-2-t
https://docs.fluentbit.io/manual/installation/kubernetes
https://sysdig.com/blog/kubernetes-security-rbac-tls/
https://sysdig.com/blog/kubernetes-security-psp-network-policy/
https://sysdig.com/blog/kubernetes-security-kubelet-etcd/
https://www.cnblogs.com/ryanyangcs/   
https://www.yellowduck.be/posts/k8s-getting-real-ip-digital-ocean/  ###ingress get real clientIP not pod GW
https://www.adaltas.com/en/2019/08/07/users-rbac-kubernetes/
https://docs.microsoft.com/en-us/azure/aks/operator-best-practices-cluster-security
https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv
https://docs.azure.cn/zh-cn/aks/azure-ad-rbac
https://docs.microsoft.com/en-us/azure/aks/api-server-authorized-ip-ranges?ocid=AID754288&wt.mc_id=azfr-c9-scottha&wt.mc_id=CFID0533
https://sysdig.com/blog/kubernetes-monitoring-prometheus-operator-part3/
https://medium.com/@xxradar/how-to-tcpdump-effectively-in-kubernetes-part-1-a1546b683d2f
https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/
https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/
https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/kubernetes-networking.md
https://livebook.manning.com/book/kubernetes-in-action/kubernetes-in-action/
https://livebook.manning.com/book/kubernetes-in-action-second-edition/welcome/v-5/
https://blog.inkubate.io/install-and-configure-a-multi-master-kubernetes-cluster-with-kubeadm/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml 
https://sysdig.com/blog/kubernetes-monitoring-prometheus-operator-part3/
https://imroc.io/
https://imroc.io/en/posts/troubleshooting-with-kubernetes-network/
https://docs.bitnami.com/tutorials/create-multi-cluster-monitoring-dashboard-thanos-grafana-prometheus/
https://github.com/luksa/kubernetes-in-action
https://yunlzheng.gitbook.io/prometheus-book/
https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
https://theojulienne.io/2019/07/06/debugging-network-latency-kubernetes.html
https://severalnines.com/database-blog/running-proxysql-kubernetes-service
https://docs.genesys.com/Documentation/GCXI/9.0.0/Dep/DockerOffline         ####Installing Kubernetes and Docker in offline scenarios
https://www.centlinux.com/2019/04/install-kubernetes-k8s-offline-on-centos-7.html
https://gist.github.com/onuryilmaz/89a29261652299d7cf768223fd61da02#download-kubernetes-rpms
https://www.techrepublic.com/article/how-to-quickly-install-kubernetes-on-ubuntu/
https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#optional-cluster-services
https://kubernetes.io/docs/concepts/cluster-administration/logging/
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
https://kubernetes.io/docs/reference/access-authn-authz/authentication/
https://kubernetes.io/docs/reference/access-authn-authz/authorization/
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/   ###stateful mysql cluster
https://my.oschina.net/u/4148359/blog/4531605 ##comparing-thanos-to-victoriametrics-cluster
https://fuckcloudnative.io/posts/comparing-thanos-to-victoriametrics-cluster/`
https://segmentfault.com/a/1190000022453911
https://github.com/kayrus/prometheus-kubernetes/blob/master/alertmanager-configmap.yaml

https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
https://www.digitalocean.com/community/tutorials/how-to-build-docker-images-and-host-a-docker-image-repository-with-gitlab
https://www.digitalocean.com/community/tutorials/how-to-set-up-continuous-integration-pipelines-with-gitlab-ci-on-ubuntu-16-04
https://dzone.com/articles/cicd-for-kubernetes-with-jenkins-and-spinnaker-con
https://spinnaker.io/guides/user/kubernetes-v2/
https://aws.amazon.com/blogs/opensource/deployment-pipeline-spinnaker-kubernetes/
https://www.metricfire.com/blog/continuous-delivery-pipeline-for-kubernetes-using-spinnaker/
https://www.linux.com/audience/enterprise/set-cicd-pipeline-kubernetes-part-1-overview/
https://www.linux.com/audience/devops/set-cicd-pipeline-jenkins-pod-kubernetes-part-2/
https://www.linux.com/training-tutorials/run-and-scale-distributed-crossword-puzzle-app-cicd-kubernetes-part-3/
https://www.linux.com/training-tutorials/set-cicd-distributed-crossword-puzzle-app-kubernetes-part-4/
https://www.weave.works/technologies/cicd-tools/
https://blog.lwolf.org/post/continuous-deployment-to-kubernetes-from-gitlab-ci/
https://blog.lwolf.org/post/how-to-build-tiny-golang-docker-images-with-gitlab-ci/
https://about.gitlab.com/blog/2017/09/21/how-to-create-ci-cd-pipeline-with-autodeploy-to-kubernetes-using-gitlab-and-helm/
https://docs.microsoft.com/en-us/azure/aks/cluster-autoscalerg
https://koudingspawn.de/kubernetes-autoscaling/

###https://linuxhandbook.com/kubectl-drain-node/
###https://www.percona.com/blog/2021/01/20/drain-kubernetes-nodes-wisely/
#Step 1: Mark the node as unschedulable (cordon)
root@kmaster-rj:~# kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
kmaster-rj    Ready    master   44d   v1.18.8
kworker-rj1   Ready    <none>   44d   v1.18.8
kworker-rj2   Ready    <none>   44d   v1.18.8
root@kmaster-rj:~#

root@kmaster-rj:~# kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
my-dep-557548758d-gprnr   1/1     Running   1          4d23h   172.16.213.48   kworker-rj1   <none>           <none>
my-dep-557548758d-d2pmd   1/1     Running   1          4d15h     172.16.213.57   kworker-rj2   <none>           <none>
pod-delete-demo           1/1     Running   1          2d      172.16.213.56   kworker-rj1   <none>           <none>
root@kmaster-rj:~#

root@kmaster-rj:~# kubectl cordon kworker-rj2
node/kworker-rj2 cordoned
root@kmaster-rj:~# 

root@kmaster-rj:~# kubectl get nodes
NAME          STATUS                     ROLES    AGE   VERSION
kmaster-rj    Ready                      master   44d   v1.18.8
kworker-rj1   Ready                      <none>   44d   v1.18.8
kworker-rj2   Ready,SchedulingDisabled   <none>   44d   v1.18.8
root@kmaster-rj:~#

#Step 2: Drain the node to prepare for maintenance
root@kmaster-rj:~# kubectl drain kworker-rj2 --grace-period=300 --ignore-daemonsets=true
node/kworker-rj2 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-fl8dl, kube-system/kube-proxy-95vdf
evicting pod default/my-dep-557548758d-d2pmd
pod/my-dep-557548758d-d2pmd evicted
node/kworker-rj2 evicted
root@kmaster-rj:~#

NOTE: kubectl drain cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet. You need to use --force to override that and by doing that the individual pods will be deleted permanently.


root@kmaster-rj:~# kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
my-dep-557548758d-gprnr   1/1     Running   1          4d23h   172.16.213.48   kworker-rj1   <none>           <none>
my-dep-557548758d-dsanh   1/1     Running   0          27s     172.16.213.38   kworker-rj1   <none>           <none>
pod-delete-demo           1/1     Running   1          2d      172.16.213.56   kworker-rj1   <none>           <none>
root@kmaster-rj:~#

#Step 3: Uncordon the node after maintenance completes

root@kmaster-rj:~# kubectl uncordon kworker-rj2
node/kworker-rj2 uncordoned

root@kmaster-rj:~# kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
kmaster-rj    Ready    master   44d   v1.18.8
kworker-rj1   Ready    <none>   44d   v1.18.8
kworker-rj2   Ready    <none>   44d   v1.18.8


###install kubernetes cluster offline ,WORKING
 
$>  vim /etc/hosts
$>  hostnamectl set-hostname lan-k8s-m01  ###host need add to /etc/hosts
$>  setenforce 0   #disable selinux
$>  vi /etc/selinux/config 
$>  yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine
$>  yum install -y yum-utils   device-mapper-persistent-data   lvm2
$>  yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
$>  yum makecache fast
$>  yum install docker-ce-18.09.9 docker-ce-cli-18.09.9 
$>  
$>  yum install -y --cacheonly --disablerepo=* ./*.rpm      ###offline install 
$>  
$>  systemctl start docker
$>  swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
$>  
$>  ls |grep -v rpm|xargs -P 1 -n 1 sh -c 'docker load <$1' _  ###offline load image
$>  
$>  docker tag 8454cbe08dc9 hub.abc.com/it/kube-proxy:v1.16.2       ###tag images
$>  docker tag 6e4bffa46d70 hub.abc.com/it/kube-scheduler:v1.16.2
$>  docker tag 8454cbe08dc9 hub.abc.com/it/kube-proxy:v1.16.2
$>  docker tag c2c9a0406787 hub.abc.com/it/kube-apiserver:v1.16.2
$>  docker tag ebac1ae204a2 hub.abc.com/it/kube-scheduler:v1.16.2
$>  docker tag 6e4bffa46d70 hub.abc.com/it/kube-controller-manager:v1.16.2
$>  docker tag b2756210eeab hub.abc.com/it/kube-controller-manager:3.3.15-0
$>  docker tag b2756210eeab hub.abc.com/it/etcd:3.3.15-0
$>  docker tag bf261d157914 hub.abc.com/it/coredns:1.6.2
$>  docker tag da86e6ba6ca1 hub.abc.com/it/pause:3.1
$>  
$>  systemctl stop firewalld  ###initial
$>  kubeadm init --image-repository=hub.abc.com/it --pod-network-cidr=10.1.0.0/16 --upload-certs --kubernetes-version=1.16.2 
$>  
$>  mkdir -p $HOME/.kube
$>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$>  sudo chown $(id -u):$(id -g) $HOME/.kube/config
$>  
$>  kubectl apply -f kube-flannel.yml ###master is read after install network plugin 
$>  kubectl get nodes  
$>     
$>  kubeadm token generate    ###create new token for join on master
$>  kubeadm token create 62f6a8.85flm1yzgr0f6r2n --print-join-command 

###renew cert
https://www.ibm.com/docs/en/fci/1.1.0?topic=kubernetes-renewing-cluster-certificates  ###this work
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/_print/
https://cloud.tencent.com/developer/article/1744610

$ kubeadm alpha certs check-expiration

$ kubeadm alpha certs renew all
$ stat /etc/kubernetes/admin.conf  ###check update date
$ cp /etc/kubernetes/admin.conf config     ###working on 1.16

$ export KUBECONFIG=/root/.kube/config



###hpa v2beta2
kubectl explain hpa --recursive=true --api-version=autoscaling/v2beta2
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
https://granulate.io/kubernetes-autoscaling-the-hpa/


apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
 name: nginx-hpa
 namespace: myspace
spec:
 behavior:
   scaleDown:
     policies:
     - type: Pods
       value: 4
       periodSeconds: 60
     - type: Percent
       value: 10
       periodSeconds: 60
   scaleUp:
     stabilizationWindowSeconds: 10
     policies:
       - type: Percent
         value: 100
         periodSeconds: 15
 scaleTargetRef:
   apiVersion: apps/v1
   kind: Deployment
   name: nginx
 minReplicas: 1
 maxReplicas: 10
 metrics:
 - type: Resource
   resource:
     name: cpu
     target:
       type: Utilization
       averageUtilization: 50
 - type: Resource
   resource:
     name: memory
     target:
       type: AverageValue
       averageValue: 100Mi
 # Uncomment these lines if you create the custom packets_per_second metric and
 # configure your app to export the metric.
 # - type: Pods
 #   pods:
 #     metric:
 #       name: packets_per_second
 #     target:
 #       type: AverageValue
 #       averageValue: 100



###yaml basic
https://developer.ibm.com/tutorials/yaml-basics-and-usage-in-kubernetes/
https://www.mirantis.com/blog/introduction-to-yaml-creating-a-kubernetes-deployment/


--template from kubectl expalin ingress --recursive=true
spec <Object>
  backend   <Object>
     serviceName    <string>
     servicePort    <string>
  rules     <[]Object>                   ###60 Plural item is list,it's first child item add prefix -
     host   <string>                     ###10 the first object's <[]Object> (rules)  propert need add prefix -   
     http   <Object>                     ###20 the other object's <[]Object>  (rules) propert not need add prefix -
        paths       <[]Object>           ###30 the non-arry object's <Object>(http)  propert not need add prefix -
           backend  <Object>
              serviceName   <string>
              servicePort   <string>
           path     <string>
  tls       <[]Object>
     hosts  <[]string>                   ####40 As a tls array,add prefix -,append new line,add prefix - and host string 
     secretName     <string>             ####50 KEY-VALUE PAIR
status       <Object>
  loadBalancer      <Object>
     ingress        <[]Object>
        hostname    <string>
        ip  <string>


--render result
spec:
 tls:
   - hosts:                                  ####40 As a tls array,add prefix -,append new line,
     - c2.yourdomain.cn                      ####40 add prefix - and host string 
     secretName: poc-tls-yourdomain-cn       ####50 KEY-VALUE PAIR 
 rules:                                      ###60 Plural item's is list,it's  FIRST child item add prefix -
   - host: c2.yourdomain.cn                  ###10 the first array object's <[]Object> (rules) propert need add prefix -
     http:                                   ###20 the other array object's <[]Object>  (rules) propert not need add prefix -
       paths:                                ###30 the non-arry object's <Object> (http) propert not need add prefix -
         - path: /
           backend:
             serviceName: my-web
             servicePort: 80



--other example

containers  <[]Object>
  ......
 image    <string>
 imagePullPolicy  <string>
 lifecycle        <Object>
    postStart     <Object>
    ......
 name     <string>
 ports  <[]Object>
   containerPort       <integer>
   hostIP      <string>
   hostPort    <integer>
   name        <string>
   protocol    <string>
 readinessProbe <Object>
   exec        <Object>
      command  <[]string>



---
apiVersion: v1
kind: Pod
metadata:
 name: rss-site
 labels:
   app: web
spec:
 containers:                         ###70 list
   - name: front-end                 ###containers child 1
     image: nginx                    ###DO NOT PREFIX -
     ports:
       - containerPort: 80
   - name: rss-reader                ###containers child 2
     image: nickchase/rss-php-nginx:v1   ###DO NOT PREFIX -
     ports:
       - containerPort: 88   ###port list,prefix -
###yaml basic

###get namespace and secret name of ingress
kubectl get ing -n pc web-ing -o yaml|grep -E 'namespace|secretName'|grep -v ","|grep -v extensions|sed -e 's:\r::g'|sed -e ':a' -e 'N' -e '$!ba' -e 's/\n/ /g' -e 's%namespace:%%g' -e 's%secretName:%%'

##namespace   secretName
  poc      poc-tls-yourdomain-cn

###https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/
kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'   ###set non-default
kubectl patch storageclass gold -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'        ###set default

###hpa autosacling
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
https://granulate.io/kubernetes-autoscaling-the-hpa/
http://www.iceyao.com.cn/2020/05/20/K8s-hpa-implement_principle/

$> cat Dockerfile.yaml
FROM php:5-apache
COPY index.php /var/www/html/index.php
RUN chmod a+rx index.php

$> cat index.php
<?php
 $x = 0.0001;
 for ($i = 0; $i <= 1000000; $i++) {
   $x += sqrt($x);
 }
 echo "OK!";
?>

$> docker build -t autoscaling:0.1

$ cat php-apache.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: php-apache
 namespace: poc
spec:
 selector:
   matchLabels:
     run: php-apache
 replicas: 1
 template:
   metadata:
     labels:
       run: php-apache
   spec:
     containers:
     - name: php-apache
       image: dockerhub.mysite.com/autoscaling:0.1
       ports:
       - containerPort: 80
       resources:
         limits:
           cpu: 500m
			memory: 200Mi
         requests:
           cpu: 200m
			memory: 100Mi
---
apiVersion: v1
kind: Service
metadata:
 name: php-apache
 namespace: poc
 labels:
   run: php-apache
spec:
 ports:
 - port: 80
 selector:
   run: php-apache
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
 name: php-apache
 namespace: poc
spec:
 scaleTargetRef:
   apiVersion: apps/v1
   kind: Deployment
   name: php-apache
 minReplicas: 1
 maxReplicas: 10
 targetCPUUtilizationPercentage: 50


#---
#apiVersion: autoscaling/v2beta2     ###support cpu/mem 
#kind: HorizontalPodAutoscaler
#metadata:
#  name: php-apache
#  namespace: poc
#spec:
#  scaleTargetRef:
#    apiVersion: apps/v1
#    kind: Deployment
#    name: php-apache
#  minReplicas: 1
#  maxReplicas: 10
#  - type: Resource
#    resource:
#      name: cpu
#      target:
#        type: Utilization
#        averageUtilization: 50
#  - type: Resource
#    resource:
#      name: memory
#      target:
#        type: AverageValue
#        averageValue: 100Mi


$> kubectl apply -f php-apache.yaml

##add pressure,revoke after create new pods by Ctrl+c
$> kubectl run -n poc -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"


$ kubectl get hpa -n poc                   ###up...
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   68%/50%   1         10        5          2m53s

$ kubectl get hpa -n poc                  ###up...
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   58%/50%   1         10        6          3m42s

$ kubectl get hpa -n poc                  ###hold...
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        6          6m24s

$ kubectl get hpa -n poc                  ###down
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        1          10m




##use azure-file on kubernetes
https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv
--checked on bitnami mariadb,mongodb;mongodb passed on recreate ,maridb not passed 

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
 name: my-azurefile
provisioner: kubernetes.io/azure-file
mountOptions:
 - dir_mode=0777
 - file_mode=0777
 - uid=1001                 ###your pod uid
 - gid=1001                 ###your pod gid
 - mfsymlinks
 - cache=strict
 - actimeo=30
parameters:
 skuName: Standard_LRS

--set at least 100GB for pod ##??


#https://docs.microsoft.com/en-us/azure/aks/ingress-own-tls     ###create Kubernetes secret for the TLS certificate
$>  kubectl create secret tls aks-ingress-tls \
   --namespace ingress-basic \
   --key aks-ingress-tls.key \
   --cert aks-ingress-tls.crt


###Access To Your Cluster With A Client CertificateAccess To Your Cluster With A Client Certificate
https://www.adaltas.com/en/2019/08/07/users-rbac-kubernetes/
https://medium.com/better-programming/k8s-tips-give-access-to-your-clusterwith-a-client-certificate-dfb3b71a76fe
https://dev.to/ajitvedpathak/user-authentication-and-authorization-in-kubernets-access-control-in-kubernetes-3ef1
https://kubernetes.io/docs/reference/access-authn-authz/rbac/ 
https://docs.microsoft.com/en-us/azure/aks/azure-ad-rbac
https://docs.microsoft.com/en-us/azure/aks/operator-best-practices-identity
https://medium.com/microsoftazure/azure-kubernetes-service-aks-authentication-and-authorization-between-azure-rbac-and-k8s-rbac-eab57ab8345d
https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/

simple check:
$ kubectl get csr Bob -o jsonpath='{.status.certificate}' | base64 --decode > Bob.crt
$ kubectl config set-credentials Bob --client-certificate=Bob.crt --client-key=Bob.key


$ openssl genrsa -out dave.key 4096         ###generate private key

$ cat <<eof >csr.cnf
[ req ]
default_bits = 2048
prompt = no
default_md = sha256
distinguished_name = dn
[ dn ]
CN = dave
O = dev
[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth          ####Note: the clientAuth entry in the extendedKeyUsage field is important 
                                                ###as the certificate will be used to identify the client.
eof

$ openssl req -config ./csr.cnf -new -key dave.key -nodes -out dave.csr

$ cat <<eof >csr.yaml
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
 name: mycsr
spec:
 groups:
 - system:authenticated
 request: ${BASE64_CSR}
 usages:
 - digital signature
 - key encipherment
 - server auth
 - client auth
eof

# Encoding the .csr file in base64
$ export BASE64_CSR=$(cat ./dave.csr | base64 | tr -d '\n')
# Substitution of the BASE64_CSR env variable and creation of the CertificateSigninRequest resource
$ cat csr.yaml | envsubst | kubectl apply -f -

# Checking the status of the newly created CSR
$ kubectl get csr
NAME        AGE   REQUESTOR            CONDITION
mycsr       9s    28b93...d73801ee46   Pending

$ kubectl certificate approve mycsr

$ kubectl get csr
NAME        AGE   REQUESTOR            CONDITION
mycsr       9s    28b93...d73801ee46   Approved,Issued

$ kubectl get csr mycsr -o jsonpath='{.status.certificate}' \
 | base64 --decode > dave.crt

$ openssl x509 -in ./dave.crt -noout -text
Certificate:
           ...
			Not After : Jun  2 07:56:00 2020 GMT
       Subject: O=dev, CN=dave         #####key point,CN is userName,O is Group id.
...

$ kubectl create ns development

$ echo <<eof |kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: development
  name: dev
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["create", "get", "update", "list", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["create", "get", "update", "list", "delete"]
eof

$ echo <<eof |kubectl apply -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev
  namespace: development
subjects:
- kind: User
  name: dave
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev
  apiGroup: rbac.authorization.k8s.io
eof

###or binding to group(last one not applied)
$ echo <<eof |kubectl apply -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev
  namespace: development
subjects:
- kind: Group
  name: dev   ###the group information is provided in the Organisation (O) field within the certificate that is sent with each request.
 apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev
  apiGroup: rbac.authorization.k8s.io
eof

#create a kubeconfig.tpl file
$ cat  <<eof >kubeconfig.tpl
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: ${CLUSTER_CA}
    server: ${CLUSTER_ENDPOINT}
  name: ${CLUSTER_NAME}
users:
- name: ${USER}
  user:
    client-certificate-data: ${CLIENT_CERTIFICATE_DATA}
contexts:
- context:
    cluster: ${CLUSTER_NAME}
    user: ${USER}
  name: ${USER}-${CLUSTER_NAME}
current-context: ${USER}-${CLUSTER_NAME}
eof

# User identifier
$ export USER="dave"
# Cluster Name (get it from the current context)
$ export CLUSTER_NAME=$(kubectl config view --minify -o jsonpath={.current-context})
# Client certificate
$ export CLIENT_CERTIFICATE_DATA=$(kubectl get csr mycsr -o jsonpath='{.status.certificate}')
# Cluster Certificate Authority
$ export CLUSTER_CA=$(kubectl config view --raw -o json | jq -r '.clusters[] | select(.name == "'$(kubectl config current-context)'") | .cluster."certificate-authority-data"')
# API Server endpoint
$ export CLUSTER_ENDPOINT=$(kubectl config view --raw -o json | jq -r '.clusters[] | select(.name == "'$(kubectl config current-context)'") | .cluster."server"')

$ cat kubeconfig.tpl | envsubst > kubeconfig

$ export KUBECONFIG=$PWD/kubeconfig

#add his private key, dave.key generated at the beginning of the process
$ kubectl config set-credentials dave \
 --client-key=$PWD/dave.key \
 --embed-certs=true

$ kubectl version   ###If everything is fine, Dave should be able to check the version of the server
Client Version: version.Info{Major:"1", Minor:"14 ......
Server Version: version.Info{Major:"1", Minor:"14 ......

$ kubectl auth can-i list pods -n development --as Bob  ###!!!!Important check,result is Yes/No,need Administrator privileges

$ kubectl get nodes  ### no privileges ,can not get result
Error from server (Forbidden): nodes is forbidden: User "dave" cannot list resource "nodes" in API group "" at the cluster scope

$ echo <<eof|kubectl apply -f -  ###It's working as has privilges
apiVersion: apps/v1
kind: Deployment
metadata:
 name: www
 namespace: development
spec:
 replicas: 3
 selector:
   matchLabels:
     app: www
 template:
   metadata:
     labels:
       app: www
   spec:
     containers:
     - name: nginx
       image: nginx:1.14-alpine
       ports:
       - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
 name: www
 namespace: development
spec:
 selector:
   app: vote
 type: ClusterIP
 ports:
 - port: 80
   targetPort: 80
eof

$ echo <<eof|kubectl apply -f -  ###no privileges,cannot create
# credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
  namespace: development
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm




###sniffer tcpdump
$ vim patch.yaml
spec:
 template:
   spec:
     containers:
     - name: tcpdumper
       image: docker.io/dockersec/tcpdump
       securityContext:
         allowPrivilegeEscalation: false
         runAsUser: 0

$ kubectl patch deployment radarhack-deployment --patch “$(cat patch.yaml)”  ###apply


####debug dns error
https://v1-16.docs.kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/

$> kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml
pod/busybox created

$> kubectl get pods busybox
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          <some-time>


$> kubectl exec -ti busybox -- nslookup kubernetes.default

$> kubectl exec busybox cat /etc/resolv.conf
$> kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
$ kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...

$ kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                    READY     STATUS    RESTARTS   AGE
...
kube-dns-v19-ezo1y      3/3       Running   0           1h
...

$ for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done

.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c

$> kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c kubedns

$> kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c dnsmasq

$> kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c sidecar

$ kubectl get svc --namespace=kube-system
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      <none>        53/UDP,53/TCP        1h
...

pod $>echo >/dev/tcp/10.96.0.10/53       ###$?=0 is ok ,or error,ip is equal kube-dns.kube-system next
$ echo >/dev/tcp/kube-dns.kube-system/53 ###centos firewalld deny coredns,open it's port not work,WORNG PORT?

#####rejoin master to cluster
//10 get command and ca key on normal master
$ sudo kubeadm token create --print-join-command            ####get join-command on normal master
kubeadm join haproxyIP:6443 --token iy534.dk     --discovery-token-ca-cert-hash sha256:8d91...354

$ sudo  kubeadm init phase upload-certs --upload-certs       ####get join ca key on normal master
W1113 10:37:56.760319   20588 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1113 10:37:56.760418   20588 version.go:102] falling back to the local client version: v1.16.2
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
412b...588      ######ca key

//20 join master on malfunction master
$ sudo  kubeadm reset        #####clean master info on malfunction master,then exec next join command with control-plane parameter
$ sudo  kubeadm join haproxyIP:6443 --token iy534.dk \    
--discovery-token-ca-cert-hash sha256:8d91...354 \
--control-plane --certificate-key 412b...588  \
--kubernetes-version=v1.17.2                    ####for match version
#####rejoin master to cluster

firewalld $>echo "8285/udp 8472/udp 43170/tcp 10100-10300/tcp 2300-2500/tcp 30000-32767/tcp 9100/tcp"|sed -e 's:\ :\n:g'|xargs -P 1 -n 1 -I {} -t sh -c 'firewall-cmd --add-port {} --perm' _



$online> docker pull k8s.gcr.io/kube-apiserver:v1.18.1 
$online> docker save k8s.gcr.io/kube-apiserver:v1.18.1 > kube-apiserver_v1.18.1.tar
$online> docker save --output api-server.tar  api-server-hash   ###This work

$offline> docker load < kube-apiserver_v1.18.1.tar
fe9a8b4f1dcc: Loading layer [==================================================>]  43.87MB/43.87MB
15c9248be8a9: Loading layer [==================================================>]  3.403MB/3.403MB
d2956a2953c6: Loading layer [==================================================>]  40.65MB/40.65MB
Loaded image: gcr.api.com/google_containers/kube-proxy:v1.16.2

###use subPathExpr 
https://kubernetes.io/docs/concepts/storage/volumes/

apiVersion: v1
kind: Pod
metadata:
 name: pod1
spec:
 containers:
 - name: container1
   env:                           ###set every container env if in statefulset
   - name: POD_NAME
     valueFrom:
       fieldRef:
         apiVersion: v1
         fieldPath: metadata.name
   image: busybox
   command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ]
   volumeMounts:
   - name: workdir1
     mountPath: /logs              ###The host directory /var/log/pod1 is mounted at /logs in the container.
     subPathExpr: $(POD_NAME)      ###The subPath and subPathExpr properties are mutually exclusive.
 restartPolicy: Never
 volumes:
 - name: workdir1
   hostPath:
     path: /var/log/


###restrict pod schedule to specfic node
https://www.decodingdevops.com/kubernetes-node-affinity-example/
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
https://ithelp.ithome.com.tw/articles/10221929
https://medium.com/faun/node-affinity-in-kubernetes-75777bf02adb

$> kubectl label nodes k8s-n1 disktype=ssd
$> kubectl taint nodes k8s-n1 non_projA=value:NoSchedule
$> kubectl taint nodes k8s-n1 non_projA=value:NoSchedule- ###undo taint
$> kubectl describe node k8s-n1 |grep non_projA
Taints:             non_projA=value:NoSchedule
$> kubectl get nodes --show-labels|grep ssd
k8s-n1   Ready   ...... disktype=ssd,kubernetes.io/arch=amd64 ......

apiVersion: v1
kind: Pod
metadata:
 name: nginx
 labels:
   env: test
spec:
 containers:
 - name: nginx
   image: dlr.abc.com/nginx:1.19.0-debian-10-r2
   imagePullPolicy: IfNotPresent
 nodeSelector:
   disktype: ssd
#---   ###nodeAffinity 
#apiVersion: apps/v1
#kind: Deployment
#metadata:
#  name: nginx-deployment
#spec:
#  replicas: 2
#  selector:
#    matchLabels:
#      app: nginx
#  template:
#    metadata:
#      labels:
#        app: nginx
#    spec:
#      affinity:
#        nodeAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#            nodeSelectorTerms:
#            - matchExpressions:
#              - key: disktype
#                operator: In
#                values:
#                - hdd 
#      containers:
#      - name: nginx
#        image: repo.a.com/bitnami/nginx:1.19.0-debian-10-r2
#        ports:
#        - containerPort: 80


$> kubectl apply -f nginx.yaml

$> kubectl describe  po nginx
Name:         nginx
Namespace:    default
Priority:     0
Node:         <none>
Labels:       env=test
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
......
Events:
 Type     Reason            Age                From               Message
 ----     ------            ----               ----               -------
 Warning  FailedScheduling  76s (x7 over 10m)  default-scheduler
 
 0/3 nodes are available:
 1 node(s) had taints that the pod didn't tolerate,
 2 node(s) didn't match node selector.

####helm yaml settings
nodeSelector:              ###spec 
 proj: bms 
tolerations:               ###spec
 - key: env
   effect: NoSchedule
   operator: Exists


###https://computingforgeeks.com/join-new-kubernetes-worker-node-to-existing-cluster/
https://www.serverlab.ca/tutorials/containers/kubernetes/how-to-add-workers-to-kubernetes-clusters/
How to Add Workers to Kubernetes Clusters

--10 list your current tokens on the Master node
master >$ kubeadm token list

--20 if not present ,create one

master >$ kubeadm token create --print-join-command

--30 Get Discovery Token CA cert Hash
master >$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

--40 Get API Server Advertise address
master >$ Kubernetes master is running at https://192.168.122.195:6443
KubeDNS is running at https://192.168.122.195:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://192.168.122.195:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

--50 Join a new Kubernetes Worker Node a Cluster
node >$ kubeadm join \
 <control-plane-host>:<control-plane-port> \
   --token <token> \
     --discovery-token-ca-cert-hash sha256:<hash>


## Removing a Worker Node from the Cluster
--10 Migrate pods from the node:
master >$ kubectl drain  <node-name> --delete-local-data --ignore-daemonsets

--20 Prevent a node from scheduling new pods use – Mark node as unschedulable

master >$ kubectl cordon <node-name>

--30 Revert changes made to the node by ‘kubeadm join‘ – Run on worker node to be removed

$node >$ kubeadm reset

Q: “Error: found in requirements.yaml, but missing in charts/ directory” Satisfying Helm Chart Dependencies
A: helm dependency update
R: https://andrewaadland.me/2018-08-10-error-found-in-requirements-yaml-but-missing-in-charts-directory-helm-chart-dependencies/

$> kubectl port-forward pod/PODNAME LOCALHOST_PORT:POD_PORT -n NAMESPACE         ###FORWARD TO LOCALHOST

$> kubectl port-forward --address ethIP service/SVC_NAME 53306:3306 -n NAMESPACE

$> kubectl exec -i -t -n namespaceName pod-Name -c containName "--" sh -c "clear; (bash || ash || sh)"

###
Create and Limit Service account to a namespace in Kubernetes
https://computingforgeeks.com/restrict-kubernetes-service-account-users-to-a-namespace-with-rbac/

Step 1: Create a namespace

$ kubectl create namespace demo   
namespace/demo created

$ kubectl get namespaces
NAME              STATUS   AGE
demo              Active   24s

Step 2: Create a Service Account
$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
 name: demo-user
 namespace: demo
EOF


Step 3: Create a Role
$ kubectl api-versions| grep  rbac       ###confirm API versions for RBAC available in your Kubernetes cluster:      
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1

Let’s create a role which will give created account complete access to namespace resources.
cat <<EOF | kubectl apply -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: admin
 namespace: demo
rules:
- apiGroups: ["", "extensions", "apps"]
 resources: ["*"]
 verbs: ["*"]
- apiGroups: ["batch"]
 resources:
 - jobs
 - cronjobs
 verbs: ["*"]
EOF


Confirm creation:
$ kubectl get roles -n demo          
NAME                   AGE
admin   94s

A role can also be created with limited access to resources in a namespace, example:
cat <<EOF | kubectl apply -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 namespace: demo
 name: deployment-admin
rules:
- apiGroups: ["", "extensions", "apps"]
 resources: ["deployments", "replicasets", "pods", "services", "ingresses"]
 verbs: ["get", "list", "watch", "create", "update", "patch", "delete"] # You can also use ["*"]
EOF

Step 4: Bind the role to a user
cat <<EOF | kubectl apply -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: admin-view
 namespace: demo
subjects:
- kind: ServiceAccount
 name: demo-user
 namespace: demo
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: Role
 name: admin
EOF

Check user token name:
$ kubectl describe sa demo-user -n demo
Name:                demo-user
Namespace:           demo
Labels:              
Annotations:         kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"demo-user","namespace":"demo"}}
Image pull secrets:  
Mountable secrets:   demo-user-token-k9qbl
Tokens:              demo-user-token-k9qbl
Events:              


Get service account token to be used to access Kubernetes on dashboard or through kubectl command line.
$ export NAMESPACE="demo"
$ export K8S_USER="demo-user"
$ kubectl -n ${NAMESPACE} describe secret $(kubectl -n ${NAMESPACE} get secret | (grep ${K8S_USER} || echo "$_") | awk '{print $1}') | grep token: | awk '{print $2}'\n

Get certificate data
$ kubectl  -n ${NAMESPACE} get secret `kubectl -n ${NAMESPACE} get secret | (grep ${K8S_USER} || echo "$_") | awk '{print $1}'` -o "jsonpath={.data['ca\.crt']}"

Step 5: Creating kubectl configuration
$ cat .kube/config
apiVersion: v1
clusters:
- cluster:
   certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJXRENCL3FBREFnRUNBZ0VBTUFvR0NDcUdTTTQ5QkFNQ01DTXhJVEFmQmdOVkJBTU1HR3N6Y3kxelpYSjIKWlhJdFkyRkFNVFUzTmpNd01qVXdNakFlRncweE9URXlNVFF3TlRRNE1qSmFGdzB5T1RFeU1URXdOVFE0TWpKYQpNQ014SVRBZkJnTlZCQU1NR0dzemN5MXpaWEoyWlhJdFkyRkFNVFUzTmpNd01qVXdNakJaTUJNR0J5cUdTTTQ5CkFnRUdDQ3FHU000OUF3RUhBMElBQlBiSmdKQ2w0elZsNFlaQUJ4dThnbFk1c2hEeDYwaVd6cXY0RU96MS93K24KKzJpMG5heUxuRTF1MjZmT1ZkY3dlMFdjUFJCb2J0M2ViNnNtYWRKQUhBV2pJekFoTUE0R0ExVWREd0VCL3dRRQpBd0lDcERBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFEczN4YTArK0E4CktVd0w2NUZyR25vWW9sTWNUei81QnoxSmNJc292VkxncUFJaEFJV0xRUXpyWkpDem9TaVlEMFZvUXhlaXIwOUEKVWhnTDBucFlzRDVUUlVYKwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
   server: https://k3s-master01:6443
 name: mycluster


contexts:
- context:
   cluster: mycluster
   namespace: demo
   user: demo-user
 name: demo

current-context: demo
kind: Config
preferences: {}


users:
- name: demo-user
 user:
   token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkRrUEFveUZGUGZZS0Q3Tzl5eVZpcFE5elFYZEI5SWZ6ZlVhYXFzLU04ZTQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlbW8tdXNlci10b2tlbi1rOXFibCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZW1vLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjOWRhNGVmOC1jNmQ5LTQ0NTEtYTQ5Ny02ODc1MjY1MzAwMzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVtbzpkZW1vLXVzZXIifQ.CnrAkziL_Qr8QNQCV_PDkXCi2H-4MoPUGoPVxjSWGZUTXd6V-a9_JKv6t5Vqhrh5vXNTkDSaR1BtLCpKYdXTyqY6CjbyI7gYYcA2M22nkCDjUiwDhxInlios29SAtoOAXq7rwg_cdgdA7XWAWEcWDtT1vRe5LLbXsnORuJ5BtYXynQXWjWjbcC6T9XqRL7iZX4VUk4YCAkX7N89OGzvyycUjjHzOne67qzqOawzjYqeSzHiXIXILwHk4KKhU8tdGG6shYF7niazdp6ZyssdQ24lQext9jzDeUZf3iXPJ_bvZUv4Jo0_eZjldi9WW0dgN5PXe5r-cD1nOJHE8sClBsg
   client-key-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JS

Let’s confirm that it works:
$ kubectl get secrets
NAME                    TYPE                                  DATA   AGE
default-token-25lbj     kubernetes.io/service-account-token   3      55m
demo-user-token-k9qbl   kubernetes.io/service-account-token   3      50m

$ kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User "system:serviceaccount:demo:demo-user" cannot list resource "nodes" in API group "" at the cluster scope

###
devops.stackexchange.com/questions/4013/how-to-pull-a-docker-image-from-a-private-docker-registry-using-helm

$> less values.yaml

imageCredentials:
 name: credentials-name
 registry: private-docker-registry
 username: user
 password: pass

$> cat templates/imagePullSecret.yaml
{{- define "imagePullSecret" }}
{{- printf "{\"auths\": {\"%s\": {\"auth\": \"%s\"}}}" .Values.imageCredentials.registry (printf "%s:%s" .Values.imageCredentials.username .Values.imageCredentials.password | b64enc) | b64enc }}
{{- end }}

$> cat templates/secret.yaml         ####add below snippy
---
apiVersion: v1
kind: Secret
metadata:
 name: {{ .Values.imageCredentials.name }}
type: kubernetes.io/dockerconfigjson
data:
 .dockerconfigjson: {{ template "imagePullSecret" . }}

$> cat templates/deployment.yaml  or templates/statefulset.yaml
   imagePullSecrets:                                          ###PATT: LEVEL
   - name: {{ .Values.imageCredentials.name }} 
   initContainers:



####persistentVolumn pv nfs NEED INSTALL NFS-UTIL ON NODE FIRST(OMITED HERE)
https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-volumes-example-nfs-persistent-volume.html
https://lapee79.github.io/en/article/use-a-local-disk-by-local-volume-static-provisioner-in-kubernetes/
https://vocon-it.com/2018/12/20/kubernetes-local-persistent-volumes/

--05 cggroupdriver settings

echo > /etc/docker/daemon.json << eof
{
 "exec-opts": ["native.cgroupdriver=systemd"],
 "log-driver": "json-file",
 "log-opts": {
   "max-size": "100m"
 },
 "storage-driver": "overlay2"
}
eof

--10 m1 is master n1 is node
[k8s-m1]
apiVersion: apps/v1
kind: Deployment
metadata:
 annotations:
   deployment.kubernetes.io/revision: "3"
 generation: 3
 labels:
   app: nfs-client-provisioner
 name: nfs-client-provisioner
 namespace: default
spec:
 replicas: 1
 selector:
   matchLabels:
     app: nfs-client-provisioner
 strategy:
   type: Recreate
 template:
   metadata:
     creationTimestamp: null
     labels:
       app: nfs-client-provisioner
   spec:
     containers:
     - env:
       - name: PROVISIONER_NAME
         value: fuseim.pri/ifs
       - name: NFS_SERVER
         value: ip-fqcn-of-srv
       - name: NFS_PATH
         value: /data
       image: repo.myorg.com/nfs-client-provisioner:201809
       imagePullPolicy: Always
       name: nfs-client-provisioner
       resources: {}
       terminationMessagePath: /dev/termination-log
       terminationMessagePolicy: File
       volumeMounts:
       - mountPath: /persistentvolumes
         name: nfs-client-root
     dnsPolicy: ClusterFirst
     restartPolicy: Always
     schedulerName: default-scheduler
     securityContext: {}
     serviceAccount: nfs-client-provisioner
     serviceAccountName: nfs-client-provisioner
     terminationGracePeriodSeconds: 30
     volumes:
     - name: nfs-client-root
       nfs:
         path: /data
         server: ip-fqcn-of-srv

[k8s-m1 ]# cat storageClass.yaml                                                                                                                                                             
kind: StorageClass                                                                                                                                                                           
apiVersion: storage.k8s.io/v1                                                                                                                                                                
metadata:                                                                                                                                                                                    
 name: my-local-storage                                                                                                                                                                     
provisioner: kubernetes.io/no-provisioner                                                                                                                                                    
volumeBindingMode: WaitForFirstConsumer                                                                                                                                                      
                                                                                                                                                                                            
[k8s-m1 ]# kubectl get sc                                                                                                                                                                    
NAME               PROVISIONER                    AGE                                                                                                                                        
my-local-storage   kubernetes.io/no-provisioner   24m   

-m1 ]# cat persistentVolume.yaml                                                                                                                                                         
apiVersion: v1                                                                                                                                                                               
kind: PersistentVolume                                                                                                                                                                       
metadata:                                                                                                                                                                                    
 name: my-local-pv                                                                                                                                                                          
spec:                                                                                                                                                                                        
 capacity:                                                                                                                                                                                  
   storage: 5Gi                                                                                                                                                                             
 accessModes:                                                                                                                                                                               
 - ReadWriteOnce                                                                                                                                                                            
 persistentVolumeReclaimPolicy: Retain                                                                                                                                                      
 storageClassName: my-local-storage                                                                                                                                                         
 local:                                                                                                                                                                                     
   path: /mnt/vol1                                                                                                                                                                          
 nodeAffinity:                                                                                                                                                                              
   required:                                                                                                                                                                                
     nodeSelectorTerms:                                                                                                                                                                     
     - matchExpressions:                                                                                                                                                                    
       - key: kubernetes.io/hostname                                                                                                                                                        
         operator: In                                                                                                                                                                       
         values:                                                                                                                                                                            
         - k8s-n1                                                                                                                                                                           

[k8s-m1 ]# kubectl get pv
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS       REASON   AGE
my-local-pv   5Gi        RWO            Retain           Bound    default/my-claim   my-local-storage            18m



[k8s-m1 ]# cat persistentVolumeClaim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: my-claim
spec:
 accessModes:
 - ReadWriteOnce
 storageClassName: my-local-storage
 resources:
   requests:
     storage: 3Gi

PWD:~/study/persistentVolumn  EXIT:0 TIME:2020-0422-16:53:43CST USER:root
[k8s-m1 ]# kubectl get pvc
NAME       STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS       AGE
my-claim   Bound    my-local-pv   5Gi        RWO            my-local-storage   17m


[k8s-m1 ]# cat http-pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: www
 labels:
   name: www
spec:
 containers:
 - name: www
   image: dockerhub.abc.com/it/nginx:alpine
   ports:
     - containerPort: 80
       name: www
   volumeMounts:
     - name: www-persistent-storage
       mountPath: /usr/share/nginx/html
 volumes:
   - name: www-persistent-storage
     persistentVolumeClaim:
       claimName: my-claim

PWD:~/study/persistentVolumn  EXIT:0 TIME:2020-0422-16:54:04CST USER:root
[k8s-m1 ]# kubectl get po -o wide
NAME                  READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
www                   1/1     Running   0          4m50s   10.249.1.30   k8s-n1   <none>           <none>

--20
[root@k8s-n1 ~]# mkdir -p /mnt/vol1
[root@k8s-n1 ~]echo "Hello local persistent volume" > /mnt/disk/vol1/index.html

--30
[k8s-m1 ]# curl 10.249.1.30
Hello local persistent volume

#persistentVolumn pv



######kubectl pull private repo
https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
$k8s-master> kubectl create secret docker-registry regcred --docker-server=dockerhub.abc.com --docker-username=user --docker-password="pwd" --docker-email=a@a.com
$k8s-master> kubectl get secret regcred -o yaml
apiVersion: v1
data:
 .dockerconfigjson: eyJhdXRocyI6.........JXKfk9fQ==
kind: Secret
metadata:
 creationTimestamp: "2020-02-25T08:06:05Z"
 name: regcred
 namespace: default
 resourceVersion: "2972"
 selfLink: /api/v1/namespaces/default/secrets/regcred
 uid: a16567c5-41ea-4f35-b184-5fcc3d39c265
type: kubernetes.io/dockerconfigjson

$k8s-master> cat pullFromPrivateDockerHubRepo.yaml 
apiVersion: v1
kind: Pod
metadata:
 name: pullFromPrivateDockerHubRepo
spec:
 containers:
 - image: dockerhub.abc.com/image-here
   name: kubia
   ports:
   - containerPort: 8080
     protocol: TCP
 imagePullSecrets:            #####key point,the 2 lines 
 - name: regcred

$>k8s-master> kubectl create -f pullFromPrivateDockerHubRepo.yaml

#######install with ali ############
https://www.liammoat.com/blog/2019/using-kubectl-to-generate-kubernetes-yaml
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands
https://www.liammoat.com/blog/2019/using-kubectl-to-generate-kubernetes-yaml
$> kubectl run my-nginx --image nginx --restart Never --dry-run -o yaml
apiVersion: v1
kind: Pod
metadata:
 creationTimestamp: null
 labels:
   run: my-nginx
 name: my-nginx
spec:
 containers:
 - image: nginx
   name: my-nginx
   resources: {}
 dnsPolicy: ClusterFirst
 restartPolicy: Never
status: {}

==================docker images ver
# docker images
REPOSITORY                                                             TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy                     v1.16.2             8454cbe08dc9        3 weeks ago         86.1MB
registry.aliyuncs.com/google_containers/kube-apiserver                 v1.16.2             c2c9a0406787        3 weeks ago         217MB
registry.aliyuncs.com/google_containers/kube-controller-manager        v1.16.2             6e4bffa46d70        3 weeks ago         163MB
registry.aliyuncs.com/google_containers/kube-scheduler                 v1.16.2             ebac1ae204a2        3 weeks ago         87.3MB
quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller   0.26.1              29024c9c6e70        6 weeks ago         483MB
quay.io/kubernetes-ingress-controller/nginx-ingress-controller         0.26.1              29024c9c6e70        6 weeks ago         483MB
registry.aliyuncs.com/google_containers/etcd    



###GFW
https://github.com/Azure/container-service-for-azure-china/blob/master/aks/README.md#limitations-of-current-aks-private-preview-on-azure-china
global                     proxy in China                                                                 format  example
dockerhub (docker.io)   dockerhub.azk8s.cn      dockerhub.azk8s.cn/<repo-name>/<image-name>:<version>   dockerhub.azk8s.cn/microsoft/azure-cli:2.0.61 dockerhub.azk8s.cn/library/nginx:1.15
gcr.io  gcr.azk8s.cn    gcr.azk8s.cn/<repo-name>/<image-name>:<version>                                 gcr.azk8s.cn/google_containers/hyperkube-amd64:v1.13.5
quay.io quay.azk8s.cn   quay.azk8s.cn/<repo-name>/<image-name>:<version>                                quay.azk8s.cn/deis/go-dev:v1.10.0

=========================
useful cmd
reset/remove exist cluster
echo y|kubeadm reset && rm -rfv $HOME/.kube/config && ip link delete flannel.1                                                                 
kubeadm init --config=kubeadm-config.yaml --upload-certs

kubectl describe pod coredns-58cc8c89f4-jjrz8 -n kube-system

kubeadm init --image-repository=azk.cn/containers --pod-network-cidr=10.1.0.0/16 --upload-certs \
--apiserver-cert-extra-sans your-public-dns-or-ip.domain.com \ ###for public use
--kubernetes-version=1.16.2         ##specify version

kubectl apply -f kube-flannel.yaml  ######only run on first master

kubectl get pods -A -o wide

kubectl get nodes -A

kubectl logs -f etcd-k8s-m1  -n kube-system


kubectl get svc -o yaml


# ip a ####flannel.1 and cni0 MUST EXIST ON kubelet nodes and master
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
   link/ether 02:42:82:cb:d3:29 brd ff:ff:ff:ff:ff:ff
   inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
      valid_lft forever preferred_lft forever
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
   link/ether 2a:ce:5f:d4:1e:91 brd ff:ff:ff:ff:ff:ff
   inet 10.244.0.0/32 scope global flannel.1
      valid_lft forever preferred_lft forever
   inet6 fe80::28ce:5fff:fed4:1e91/64 scope link
      valid_lft forever preferred_lft forever
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
   link/ether da:4c:87:d2:0e:bb brd ff:ff:ff:ff:ff:ff
   inet 10.244.0.1/24 scope global cni0
      valid_lft forever preferred_lft forever

=========================
https://yq.aliyun.com/articles/702158

--5 master


systemctl stop firewalld && systemctl disable firewalld
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab

cat > /etc/sysconfig/modules/ipvs.modules <<EOF         ###not needed
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

cat <<EOF >
/etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
yum install -y yum-utils device-mapper-persistent-data lvm2
yum -y install wget
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast
yum install -y docker-ce
systemctl start docker
systemctl enable docker


cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl start kubelet
systemctl enable --now kubelet
systemctl status kubelet


mkdir /etc/docker -p
# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
 "exec-opts": ["native.cgroupdriver=systemd"],
 "log-driver": "json-file",
 "log-opts": {
   "max-size": "100m"
},
 "storage-driver": "overlay2",
 "storage-opts": [
   "overlay2.override_kernel_check=true"
 ]
}
{
 “registry-mirrors”: [“https://registry.docker-cn.com“]
}
EOF

master $> cat kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
controlPlaneEndpoint: "10.0.2.22:6443"
imageRepository: "registry.aliyuncs.com/google_containers"
networking:
 podSubnet: "10.249.0.0/16"
apiServer:
 certSANs:
 - "k8s249.example.cn"





master $> kubeadm init --config=kubeadm-config.yaml --upload-certs




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
 https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
   --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e

# kubectl get pods -A
# The connection to the server localhost:8080 was refused - did you specify the right host or port?
master$>    export KUBECONFIG=/etc/kubernetes/admin.conf

or

maser$>    mkdir -p $HOME/.kube
maser$>    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
maser$>    sudo chown $(id -u):$(id -g) $HOME/.kube/config



--10 master another way
# hostnamectl set-hostname master


# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.33.10   master
192.168.33.11   node1


# cat preENV.sh
#!/bin/bash
# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld
# 关闭SELINUX
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
# 关闭Swap
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab

# sh preENV.sh

cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

maser$>    modprobe br_netfilter
maser$>    sysctl -p /etc/sysctl.d/k8s.conf


maser$>    cat > /etc/sysconfig/modules/ipvs.modules <<EOF ###not needed
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
maser$>    chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4


maser$>    yum install -y yum-utils device-mapper-persistent-data lvm2
maser$>    wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
maser$>    yum makecache fast
maser$>    yum install -y docker-ce
maser$>    systemctl start docker
maser$>    systemctl enable docker


maser$>    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
maser$>    yum makecache fast
maser$>    yum install -y kubelet kubeadm kubectl
maser$>    systemctl enable kubelet



maser$>    vim k8sMasterImages.sh
#!/bin/bash

set -e

KUBE_VERSION=v1.14.1
KUBE_PAUSE_VERSION=3.1
ETCD_VERSION=3.3.10
CORE_DNS_VERSION=1.3.1

GCR_URL=k8s.gcr.io
ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers

images=(kube-proxy:${KUBE_VERSION}
kube-scheduler:${KUBE_VERSION}
kube-controller-manager:${KUBE_VERSION}
kube-apiserver:${KUBE_VERSION}
pause:${KUBE_PAUSE_VERSION}
etcd:${ETCD_VERSION}
coredns:${CORE_DNS_VERSION})


for imageName in ${images[@]} ; do
 docker pull $ALIYUN_URL/$imageName
 docker tag  $ALIYUN_URL/$imageName $GCR_URL/$imageName
 docker rmi $ALIYUN_URL/$imageName
done
maser$>    chmod +x k8sMasterImages.sh
maser$>    ./k8sMasterImages.sh



maser$>    kubeadm init --kubernetes-version=v1.14.1 --apiserver-advertise-address=192.168.33.10 --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.14.1
[preflight] Running pre-flight checks
   [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
   [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.33.10 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.33.10 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.33.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.502789 seconds
[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --experimental-upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 6ftxi9.gov5rsp9syw1fect
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
 https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
   --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e



maser$>    mkdir -p $HOME/.kube
maser$>    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
maser$>    sudo chown $(id -u):$(id -g) $HOME/.kube/config



maser$>    kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}


maser$>    kubectl get pod -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-fb8b8dccf-88hfr          0/1     Pending   0          6m33s   <none>          <none>   <none>           <none>
kube-system   coredns-fb8b8dccf-jmw7m          0/1     Pending   0          6m33s   <none>          <none>   <none>           <none>
kube-system   etcd-master                      1/1     Running   0          5m42s   192.168.33.10   master   <none>           <none>
kube-system   kube-apiserver-master            1/1     Running   0          5m46s   192.168.33.10   master   <none>           <none>
kube-system   kube-controller-manager-master   1/1     Running   0          5m47s   192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-46wth                 1/1     Running   0          6m32s   192.168.33.10   master   <none>           <none>
kube-system   kube-scheduler-master            1/1     Running   0          5m40s   192.168.33.10   master   <none>           <none>





maser$>    kubectl describe node master | grep Taint
Taints:             node-role.kubernetes.io/master:NoSchedule
maser$>    kubectl taint nodes master node-role.kubernetes.io/master-
node "master" untainted


maser$>    kubectl get nodes
NAME     STATUS     ROLES    AGE   VERSION
master   Ready      master   58m   v1.14.1
node1    NotReady   <none>   13s   v1.14.1

maser$>    watch kubectl get nodes
Every 2.0s: kubectl get node                                                                                                                            Sun May 12 02:48:00 2019

NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   12h   v1.14.1
node1    Ready    <none>   11h   v1.14.1

maser$>    cat processFlannelImage.sh
#!/bin/bash
docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64
maser$>    chmod +x processFlannelImage.sh
maser$>    ./processFlannelImage.sh

maser$>    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  ######only run on first master
podsecuritypolicy.extensions/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created


maser$>    kubectl get pod -A -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE    IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-fb8b8dccf-88hfr            1/1     Running   0          12h    10.244.0.3      master   <none>           <none>
kube-system   coredns-fb8b8dccf-jmw7m            1/1     Running   0          12h    10.244.0.2      master   <none>           <none>
kube-system   etcd-master                        1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-apiserver-master              1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-controller-manager-master     1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5jtfd        1/1     Running   0          11h    192.168.33.11   node1    <none>           <none>
kube-system   kube-flannel-ds-amd64-bgv9j        1/1     Running   0          11h    192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-46wth                   1/1     Running   0          12h    192.168.33.10   master   <none>           <none>
kube-system   kube-proxy-r86jk                   1/1     Running   0          11h    192.168.33.11   node1    <none>           <none>
kube-system   kube-scheduler-master              1/1     Running   0          12h    192.168.33.10   master   <none>           <none>


cat /etc/haproxy/haproxy.cfg 
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
   # to have these messages end up in /var/log/haproxy.log you will
   # need to:
   #
   # 1) configure syslog to accept network log events.  This is done
   #    by adding the '-r' option to the SYSLOGD_OPTIONS in
   #    /etc/sysconfig/syslog
   #
   # 2) configure local2 events to go to the /var/log/haproxy.log
   #   file. A line like the following can be added to
   #   /etc/sysconfig/syslog
   #
   #    local2.*                       /var/log/haproxy.log
   #
   log         127.0.0.1 local2

   chroot      /var/lib/haproxy
   pidfile     /var/run/haproxy.pid
   maxconn     4000
   user        haproxy
   group       haproxy
   daemon

   # turn on stats unix socket
   stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
   mode                    http
   log                     global
   option                  httplog
   option                  dontlognull
   option http-server-close
   option forwardfor       except 127.0.0.0/8
   option                  redispatch
   retries                 3
   timeout http-request    10s
   timeout queue           1m
   timeout connect         10s
   timeout client          1m
   timeout server          1m
   timeout http-keep-alive 10s
   timeout check           10s
   maxconn                 3000

frontend k8s-api
 bind *:6443
 mode tcp
 option tcplog
 default_backend k8s-api

backend k8s-api
 mode tcp
 option tcplog
 option tcp-check
 balance roundrobin
 default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
 server k8s-m01 192.168.200.212:6443 check
 server k8s-m02 192.168.200.213:6443 check


cat kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "192.168.200.214:6443"
networking:
 podSubnet: "10.244.0.0/16"

#######install with ali ############
--20 node


systemctl stop firewalld && systemctl disable firewalld
setenforce 0 && sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
swapoff -a && sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
vim /etc/sysctl.d/k8s.conf
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
cat > /etc/sysconfig/modules/ipvs.modules <<EOFmodprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
yum install -y yum-utils device-mapper-persistent-data lvm2
yum -y install wget
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast
#yum install -y docker-ce
yum install docker-ce.x86_64 docker-ce-cli.x86_64 -y
systemctl start docker
systemctl enable docker
systemctl status kubectl
systemctl enable kubectl
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF


## Create /etc/docker directory.
mkdir /etc/docker
# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
"exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"storage-driver": "overlay2",
"storage-opts": [
"overlay2.override_kernel_check=true"
]
}
{
“registry-mirrors”: [“https://registry.docker-cn.com“]
}
EOF

yum makecache fast
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet
systemctl status kubectl
kubeadm join 10.0.2.22:6443 --token kze0e4.x0jcxedjndu215y6     --discovery-token-ca-cert-hash sha256:a718d1ea16be1045898ef5926931f447189969888a98938502a5d4c54e5884c2



--20 another way
[root@node1 ~]$ cat k8sNodeImages.sh
#!/bin/bash

set -e

KUBE_VERSION=v1.14.1
KUBE_PAUSE_VERSION=3.1

GCR_URL=k8s.gcr.io
ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers

images=(kube-proxy-amd64:${KUBE_VERSION}
pause:${KUBE_PAUSE_VERSION})


for imageName in ${images[@]} ; do
 docker pull $ALIYUN_URL/$imageName
 docker tag  $ALIYUN_URL/$imageName $GCR_URL/$imageName
 docker rmi $ALIYUN_URL/$imageName
done
node$>    chmod +x k8sNodeImages.sh
node$>    ./k8sNodeImages.sh



node$>    kubeadm join 192.168.33.10:6443 --token 6ftxi9.gov5rsp9syw1fect \
>     --discovery-token-ca-cert-hash sha256:e474c065e6c7c69c12ec37b24cedb5745f941231c532032921553766789a4a5e
[preflight] Running pre-flight checks
   [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
node$>   




40 helm
https://www.hi-linux.com/posts/21466.html

master> tar vzxf helm-v2.16.0-linux-amd64.tar.gz
master> cd linux-amd64 && cp helm /bin && cp tiller /bin
master> kubectl create serviceaccount --namespace kube-system tiller

master> kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller


master> helm init --upgrade --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

master> kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'

master> kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount

master> kubectl -n kube-system get pods|grep tiller
master> helm repo add stable  http://mirror.azure.cn/kubernetes/charts

master>  kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount
     serviceAccount: tiller
     serviceAccountName: tiller

master>  kubectl -n kube-system get pods|grep tiller
tiller-deploy-6b5ffb6f-r4rpb     1/1     Running   0          15s

master>   helm repo add stable  http://mirror.azure.cn/kubernetes/charts
"stable" has been added to your repositories

master>   helm repo list
NAME    URL                                     
stable  http://mirror.azure.cn/kubernetes/charts
local   http://127.0.0.1:8879/charts           

master>  helm list                                                                                                                                                                   
####nothing  here is correct###

master>  helm version                                                                                                                                                                
Client: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666270cfbe734aaabf342a49bb", GitTreeState:"clean"}                                                                       
Server: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666270cfbe734aaabf342a49bb", GitTreeState:"clean"}                                                                       
master>  helm search mysql                                                                                                                                                           
NAME                                    CHART VERSION   APP VERSION     DESCRIPTION                                                                                                          
stable/mysql                            1.4.0           5.7.27          Fast, reliable, scalable, and easy to use open-source rel...                                                         
stable/mariadb                          7.0.1           10.3.20         Fast, reliable, scalable, and easy to use open-source rel...                                                         
master>  helm install stable/mysql                                                                                                                                                   
NAME:   bald-chimp                                                                                                                                                                           
LAST DEPLOYED: Tue Nov 12 03:45:16 2019                                                                                                                                                      
NAMESPACE: default                                                                                                                                                                           
STATUS: DEPLOYED                                                                                                                                                                             
RESOURCES:                                                                                                                                                                                   
==> v1/ConfigMap                                                                                                                                                                             
NAME                   AGE                                                                                                                                                                   
bald-chimp-mysql-test  0s                                                                                                                                                                    
==> v1/Deployment                                                                                                                                                                            
NAME              AGE                                                                                                                                                                        
bald-chimp-mysql  0s                                                                                                                                                                         
==> v1/PersistentVolumeClaim                                                                                                                                                                 
NAME              AGE                                                                                                                                                                        
bald-chimp-mysql  0s                                                                                                                                                                         
==> v1/Pod(related)                                                                                                                                                                          
NAME                               AGE                                                                                                                                                       
bald-chimp-mysql-6657c87fbc-994t5  0s    
                                                                                                                                                                                                                                    [3/4651]
==> v1/PersistentVolumeClaim
NAME              AGE
bald-chimp-mysql  0s

==> v1/Pod(related)
NAME                               AGE
bald-chimp-mysql-6657c87fbc-994t5  0s

==> v1/Secret
NAME              AGE
bald-chimp-mysql  0s

==> v1/Service
NAME              AGE
bald-chimp-mysql  0s

NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
bald-chimp-mysql.default.svc.cluster.local

To get your root password run:

   MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default bald-chimp-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)                                                                                        

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

   kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

   $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
   $ mysql -h bald-chimp-mysql -p

To connect to your database directly from outside the K8s cluster:
   MYSQL_HOST=127.0.0.1
   MYSQL_PORT=3306

   # Execute the following command to route the connection:
   kubectl port-forward svc/bald-chimp-mysql 3306

   mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}


master>  helm list        ####Installed chart appear ,if get dial up timeout error,check kubeadmin.yaml and flannel.yaml network subnet set is identical?
###if not ,stop docker kubelet and ip link delete cni0 and ip link delete flannel.1 then restart docker kubelet(on first master node?)
NAME            REVISION        UPDATED                         STATUS          CHART           APP VERSION     NAMESPACE                                                                    
singed-peacock  1               Tue Nov 12 03:43:17 2019        DEPLOYED        mysql-1.4.0     5.7.27          default                                                                      

master>  kubectl get svc
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
bald-chimp-mysql       ClusterIP   10.99.0.194      <none>        3306/TCP   11s
kubernetes             ClusterIP   10.96.0.1        <none>        443/TCP    20m
singed-peacock-mysql   ClusterIP   10.101.253.119   <none>        3306/TCP   2m10s

master>  nc -vz 10.96.0.1 443
Ncat: Version 7.50 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.1:443.
Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.



==create helm chart
https://docs.bitnami.com/tutorials/create-your-first-helm-chart/
https://codefresh.io/docs/docs/new-helm/helm-best-practices/

--10
helm create mychart

helm install --dry-run --debug ./mychart

helm install --dry-run --debug ./mychart --set service.internalPort=8080

helm install example ./mychart --set service.type=NodePort

modify  values.yaml
image:
repository: prydonius/todo
tag: 1.0.0
pullPolicy: IfNotPresent

helm lint ./mychart

helm install example2 ./mychart --set service.type=NodePort

helm package ./mychart

helm install example3 mychart-0.1.0.tgz --set service.type=NodePort

helm serve &

helm search local
NAME            VERSION DESCRIPTION
local/mychart   0.1.0   A Helm chart for Kubernetes

helm install example4 local/mychart --set service.type=NodePort

cat > ./mychart/requirements.yaml <<EOF
dependencies:
- name: mariadb
version: 0.6.0
repository: https://kubernetes-charts.storage.googleapis.com
EOF

helm dep update ./mychart
Hang tight while we grab the latest from your chart repositories...
...Unable to get an update from the "local" chart repository (http://127.0.0.1:8879/charts):
   Get http://127.0.0.1:8879/charts/index.yaml: dial tcp 127.0.0.1:8879: getsockopt: connection refused
...Successfully got an update from the "bitnami" chart repository
...Successfully got an update from the "incubator" chart repository
Update Complete. *Happy Helming!*
Saving 1 charts
Downloading mariadb from repo
$ ls ./mychart/charts
mariadb-0.6.0.tgz



=====nginx-ingress
https://kubernetes.github.io/ingress-nginx/deploy/  azure way
https://kubernetes.github.io/ingress-nginx/deploy/  this works,replace repo only to china
master> helm install stable/nginx-ingress --set defaultBackend.image.repository=gcr.azk8s.cn/google_containers/defaultbackend --set defaultBackend.image.tag=1.4

GFW DBG
kubectl get pods -A -o wide|grep defa

kubectl describe pod mewing-flee-nginx-ingress-controller-7db757cf94-fq2m6 -n default

kubectl logs kindly-goat-nginx-ingress-controller-bf99b859-ng4cz -n default

docker images
docker tag quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1

[flannel network]
# kubectl describe pod kube-flannel-ds-amd64-9dbxj -n kube-system
Name:         kube-flannel-ds-amd64-9dbxj
Namespace:    kube-system
Priority:     0
Node:         k8s-n2/10.0.2.22
Start Time:   Mon, 25 Nov 2019 10:11:57 +0800
.....

# kubectl get pods -n kube-system -l app=flannel
NAME                          READY   STATUS    RESTARTS   AGE
kube-flannel-ds-amd64-9dbxj   1/1     Running   3          55d
kube-flannel-ds-amd64-f5q8z   1/1     Running   5          55d
......

# kubectl get cm -n kube-system -l app=flannel
NAME               DATA   AGE
kube-flannel-cfg   2      55d

# kubectl get cm -n kube-system -o yaml kube-flannel-cfg
apiVersion: v1
data:
......
net-conf.json: |
   {
     "Network": "10.249.0.0/16",
     "Backend": {
       "Type": "vxlan"                        #######This is very import
     }
   }
kind: ConfigMap
metadata:
 annotations:
......
labels:
   app: flannel
   tier: node
 name: kube-flannel-cfg
 namespace: kube-system
 resourceVersion: "2810"
 selfLink: /api/v1/namespaces/kube-system/configmaps/kube-flannel-cfg
 uid: ddfa00fe-eb34-4fdd-8e09-18c749070453


# kubectl edit cm -n kube-system -o yaml kube-flannel-cfg
change "Type": "vxlan" to "Type": "host-gw"

# kubectl delete pod -n kube-system -l app=flannel

# kubectl get cm -n kube-system -o yaml kube-flannel-cfg
apiVersion: v1
data:
 ......
 net-conf.json: |
    {
      "Network": "10.249.0.0/16",
      "Backend": {
        "Type": "host-gw"                        #######This is very import,changed
      }
    }
kind: ConfigMap
metadata:
  annotations:
......
labels:
    app: flannel
    tier: node
  name: kube-flannel-cfg
  namespace: kube-system
  resourceVersion: "2810"
  selfLink: /api/v1/namespaces/kube-system/configmaps/kube-flannel-cfg
  uid: ddfa00fe-eb34-4fdd-8e09-18c749070453
###########
[flannel network]
# kubectl describe pod kube-flannel-ds-amd64-9dbxj -n kube-system
Name:         kube-flannel-ds-amd64-9dbxj
Namespace:    kube-system
Priority:     0
Node:         k8s-n2/10.0.2.22
Start Time:   Mon, 25 Nov 2019 10:11:57 +0800
.....

# kubectl get pods -n kube-system -l app=flannel
NAME                          READY   STATUS    RESTARTS   AGE
kube-flannel-ds-amd64-9dbxj   1/1     Running   3          55d
kube-flannel-ds-amd64-f5q8z   1/1     Running   5          55d
......

# kubectl get cm -n kube-system -l app=flannel
NAME               DATA   AGE
kube-flannel-cfg   2      55d

# kubectl get cm -n kube-system -o yaml kube-flannel-cfg
apiVersion: v1
data:
......
net-conf.json: |
   {
     "Network": "10.249.0.0/16",
     "Backend": {
       "Type": "vxlan"                        #######This is very import
     }
   }
kind: ConfigMap
metadata:
 annotations:
......
labels:
   app: flannel
   tier: node
 name: kube-flannel-cfg
 namespace: kube-system
 resourceVersion: "2810"
 selfLink: /api/v1/namespaces/kube-system/configmaps/kube-flannel-cfg
 uid: ddfa00fe-eb34-4fdd-8e09-18c749070453


# kubectl edit cm -n kube-system -o yaml kube-flannel-cfg
change "Type": "vxlan" to "Type": "host-gw"

# kubectl delete pod -n kube-system -l app=flannel

# kubectl get cm -n kube-system -o yaml kube-flannel-cfg
apiVersion: v1
data:
 ......
 net-conf.json: |
    {
      "Network": "10.249.0.0/16",
      "Backend": {
        "Type": "host-gw"                        #######This is very import,changed
      }
    }
kind: ConfigMap
metadata:
  annotations:
......
labels:
    app: flannel
    tier: node
  name: kube-flannel-cfg
  namespace: kube-system
  resourceVersion: "2810"
  selfLink: /api/v1/namespaces/kube-system/configmaps/kube-flannel-cfg
  uid: ddfa00fe-eb34-4fdd-8e09-18c749070453

###file transfer
Copy requires that tar be installed in the container image.
kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir
kubectl cp <some-pod>:/tmp/foo /tmp/bar
kubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>
kubectl cp /tmp/foo <some-namespace>/<some-pod>:/tmp/bar
https://kubectl.docs.k8s.io/pages/container_debugging/copying_container_files.html


docker cp foo.txt mycontainer:/foo.txt
docker cp mycontainer:/foo.txt foo.txt
https://linuxhandbook.com/docker-cp-example/
####create token 
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/#cmd-token-create
[k8s-m1 ]# kubeadm token generate
eut5io.190s76dl6u7qi23b

[k8s-m1 ]# kubeadm token create eut5io.190s76dl6u7qi23b --print-join-command --ttl=0
kubeadm join 10.0.2.12:6443 --token eut5io.190s76dl6u7qi23b     --discovery-token-ca-cert-hash sha256:07d685118812c8331fb938574388853a654c21c588386f1d0b663e3f5e6ef335 

[k8s-m1 ]# kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                   DESCRIPTION   EXTRA GROUPS
eut5io.190s76dl6u7qi23b   <forever>   <never>   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token



####config map
--10
[k8s-m1 ]# ls configmap-files/
my-nginx-config.conf  sleep-interval

[k8s-m1 ]# cat configmap-files/my-nginx-config.conf 
server {
   listen              80;
   server_name         www.kubia-example.com;

   gzip on;
   gzip_types text/plain application/xml;

   location / {
       root   /usr/share/nginx/html;
       index  index.html index.htm;
   }

}

[k8s-m1 ]# cat configmap-files/sleep-interval 
25


[k8s-m1 ]# kubectl create cm fortune-config --from-file=configmap-files                                                                                                                      
configmap/fortune-config created 

[k8s-m1 ]# kubectl get cm fortune-config -o yaml
apiVersion: v1
data:
 my-nginx-config.conf: |
   server {
       listen              80;
       server_name         www.kubia-example.com;

       gzip on;
       gzip_types text/plain application/xml;

       location / {
           root   /usr/share/nginx/html;
           index  index.html index.htm;
       }

   }
 sleep-interval: |
   25
kind: ConfigMap
metadata:
 creationTimestamp: "2020-04-23T04:00:01Z"
 name: fortune-config
 namespace: default
 resourceVersion: "4368184"
 selfLink: /api/v1/namespaces/default/configmaps/fortune-config
 uid: a6b0e0a5-4953-48cf-85e5-908fd8bd456c


## After vim wq :configmap/fortune-config edited


[k8s-m1 ]# kubectl exec fortune-configmap-volume -c web-server cat /etc/nginx/conf.d/my-nginx-config.conf                                                                                    
server {
   listen              80;
   server_name         www.kubia-example.com;

   gzip on;
   gzip_types text/plain application/xml;

   location / {
       root   /usr/share/nginx/html;
       index  index.html index.htm;
   }

}


[k8s-m1 ]# kubectl edit configmap fortune-config      ####After save edit,the new status will take effect after monment auto.


# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
 my-nginx-config.conf: |
   server {
       listen              80;
       server_name         www.kubia-example.com;

       gzip off;                             #######change from on to off
       gzip_types text/plain application/xml;

       location / {
           root   /usr/share/nginx/html;
           index  index.html index.htm;
       }

   }
 sleep-interval: |
   25
kind: ConfigMap
metadata:
 creationTimestamp: "2020-04-23T04:00:01Z"
 name: fortune-config
 namespace: default
 resourceVersion: "4368184"
 selfLink: /api/v1/namespaces/default/configmaps/fortune-config
 uid: a6b0e0a5-4953-48cf-85e5-908fd8bd456c



[k8s-m1 ]# kubectl exec fortune-configmap-volume -c web-server cat /etc/nginx/conf.d/my-nginx-config.conf
server {
   listen              80;
   server_name         www.kubia-example.com;

   gzip off;                           #######Wait moment,this while change to config new status
   gzip_types text/plain application/xml;

   location / {
       root   /usr/share/nginx/html;
       index  index.html index.htm;
   }

}



kubectl delete pod kubia-2-manual --grace-period=0 --force

kubectl create cm fortune-config --from-literal=sleep-interval=25



[k8s-m1 ]# cat fortune-pod-configmap-volume.yaml 
apiVersion: v1
kind: Pod
metadata:
 name: fortune-configmap-volume
spec:
 containers:
 - image: fortune:evn
   imagePullPolicy: IfNotPresent
   env:
   - name: INTERVAL
     valueFrom:
       configMapKeyRef:
         name: fortune-config
         key: sleep-interval
   name: html-generator
   volumeMounts:
   - name: html
     mountPath: /var/htdocs
 - image: nginx:alpine
   name: web-server
   volumeMounts:
   - name: html
     mountPath: /usr/share/nginx/html
     readOnly: true
   - name: config
     mountPath: /etc/nginx/conf.d
     readOnly: true
   - name: config
     mountPath: /tmp/whole-fortune-config-volume
     readOnly: true
   ports:
     - containerPort: 80
       name: http
       protocol: TCP
 volumes:
 - name: html
   emptyDir: {}
 - name: config
   configMap:
     name: fortune-config

####config map




https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/

git clone https://github.com/nginxinc/kubernetes-ingress/
cd kubernetes-ingress/deployments
git checkout v1.6.3


kubectl apply -f common/ns-and-sa.yaml
kubectl apply -f rbac/rbac.yaml
kubectl apply -f common/default-server-secret.yaml
kubectl apply -f common/nginx-config.yaml
kubectl apply -f common/custom-resource-definitions.yaml



kubectl apply -f daemon-set/nginx-ingress.yaml

kubectl get pods --namespace=nginx-ingress



kubectl apply -f ds-nginx-ingress.yaml
kubectl apply -f ing-kubia.yaml 
kubectl apply -f svc-kubia-nodeport.yaml
kubectl apply -f pod-kubia.yaml


--05 
data flow:client->haproxy->ds->ing(nginx)->svc(app)->pod(labeld)

--10 master
[root@k8s-m1 Chapter05]# kubectl get ds
NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ds-nginx-ingress   2         2         2       2            2           <none>          26m
[root@k8s-m1 Chapter05]# kubectl describe ds 
Name:           ds-nginx-ingress
Selector:       app=nginx-ingress
Node-Selector:  <none>
Labels:         <none>
Annotations:    deprecated.daemonset.template.generation: 1
               kubectl.kubernetes.io/last-applied-configuration:
                 {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"name":"ds-nginx-ingress","namespace":"default"},"spec":{"selector...
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
 Labels:           app=nginx-ingress
 Service Account:  nginx-ingress
 Containers:
  nginx-ingress:
   Image:      nginx/nginx-ingress:edge
   Port:       80/TCP                            ###container port
   Host Port:  31080/TCP                         ###node port NOT SHOWED ON NODE,BUT worked if check from other node
   Args:
     -nginx-configmaps=$(POD_NAMESPACE)/nginx-config
     -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret
   Environment:
     POD_NAMESPACE:   (v1:metadata.namespace)
     POD_NAME:        (v1:metadata.name)
.......



[root@k8s-m1 Chapter05]# kubectl get ing
NAME    HOSTS                              ADDRESS   PORTS   AGE
kubia   kubia.example.com,ab.example.com             80      8m3s
[root@k8s-m1 Chapter05]# kubectl describe ing
Name:             kubia
Namespace:        default
Address:          
Default backend:  default-http-backend:80 (<none>)
Rules:
 Host               Path  Backends
 ----               ----  --------
 kubia.example.com  
                    /   kubia-nodeport:80 (10.249.1.24:8080)
 ab.example.com     
                    /   kubia-nodeport:80 (10.249.1.24:8080)
Annotations:                                ####serviceName is KEYPOINT
 kubectl.kubernetes.io/last-applied-configuration:  {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},
 "name":"kubia","namespace":"default"},"spec":{"rules":[{"host":"kubia.example.com","http":{"paths":[{"backend":{"serviceName":"kubia-nodeport",
 "servicePort":80},"path":"/"}]}},{"host":"ab.example.com","http":{"paths":[{"backend":{"serviceName":"kubia-nodeport","servicePort":80},"path":"/"}]}}]}}

Events:
 Type    Reason          Age                   From                      Message
 ----    ------          ----                  ----                      -------
 Normal  AddedOrUpdated  5m25s (x2 over 8m9s)  nginx-ingress-controller  Configuration for default/kubia was added or updated
 Normal  AddedOrUpdated  5m25s (x2 over 8m8s)  nginx-ingress-controller  Configuration for default/kubia was added or updated






[root@k8s-m1 Chapter05]# kubectl get svc
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP        15d
kubia-nodeport   NodePort    10.107.200.168   <none>        80:31025/TCP   7m1s
[root@k8s-m1 Chapter05]# kubectl get svc kubia-nodeport
NAME             TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   NodePort   10.107.200.168   <none>        80:31025/TCP   7m12s
[root@k8s-m1 Chapter05]# kubectl describe svc kubia-nodeport
Name:                     kubia-nodeport
Namespace:                default
Labels:                   <none>
Annotations:              kubectl.kubernetes.io/last-applied-configuration:
                           {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"kubia-nodeport","namespace":"default"},"spec":{"ports":[{"port":8...
Selector:                 app=kubia                 ####KEY POINT
Type:                     NodePort
IP:                       10.107.200.168
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  31025/TCP
Endpoints:                10.249.1.24:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


[root@k8s-m1 Chapter05]# kubectl label po kubia-manual app=kubia            ###KEY POINT

[root@k8s-m1 Chapter05]# kubectl get po --show-labels
NAME                     READY   STATUS    RESTARTS   AGE   LABELS
ds-nginx-ingress-s746p   1/1     Running   0          30m   app=nginx-ingress,controller-revision-hash=6d4cbc867,pod-template-generation=1
ds-nginx-ingress-xklnp   1/1     Running   0          30m   app=nginx-ingress,controller-revision-hash=6d4cbc867,pod-template-generation=1
kubia-manual             1/1     Running   0          50m   app=kubia           ####KEY POINT





--20 haproxy

[root@k8s-m2 ha ~]# cat /etc/haproxy/haproxy.cfg |tail -n 40
......
frontend k8s-80
 bind *:80
 mode tcp 
 option tcplog
 default_backend k8s-80

backend k8s-80
 mode tcp                                                                                                                                                                                                                                                                    
 option tcplog
 option tcp-check
 balance roundrobin
 default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 
 server k8s-n02 10.0.2.22:31080 check        ###KEY POINT PORT FROM DAEMONSET
 server k8s-n01 10.0.2.21:31080 check        ###KEY POINT PORT FROM DAEMONSET  


--30 client
[root@k8s-m3 ~]# curl mk.example.com -I   ###NOT REVERSE PROXY
HTTP/1.1 404 Not Found
Server: nginx/1.17.8

[root@k8s-m3 ~]# curl ab.example.com -I   ####PREVERSE PROXY
HTTP/1.1 200 OK
Server: nginx/1.17.9

[root@k8s-m3 ~]# curl example.example.com -I ###NO DNS A RECORD
curl: (6) Could not resolve host: example.example.com; Unknown error



###create pvc with storageclass not need pv on azure
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-stress-logs
  namespace: poc-azure
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: azurefile-retain
  resources:
    requests:
      storage: 10Gi

####persistentVolumn pvp/fortune-config edited

https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-volumes-example-nfs-persistent-volume.html
https://lapee79.github.io/en/article/use-a-local-disk-by-local-volume-static-provisioner-in-kubernetes/
https://vocon-it.com/2018/12/20/kubernetes-local-persistent-volumes/

--10 m1 is master n1 is node
[k8s-m1 ]# cat storageClass.yaml                                                                                                                                                             
kind: StorageClass                                                                                                                                                                           
apiVersion: storage.k8s.io/v1                                                                                                                                                                
metadata:                                                                                                                                                                                    
 name: my-local-storage                                                                                                                                                                     
provisioner: kubernetes.io/no-provisioner                                                                                                                                                    
volumeBindingMode: WaitForFirstConsumer                                                                                                                                                      
                                                                                                                                                                                            
[k8s-m1 ]# kubectl get sc                                                                                                                                                                    
NAME               PROVISIONER                    AGE                                                                                                                                        
my-local-storage   kubernetes.io/no-provisioner   24m   

-m1 ]# cat persistentVolume.yaml                                                                                                                                                         
apiVersion: v1                                                                                                                                                                               
kind: PersistentVolume                                                                                                                                                                       
metadata:                                                                                                                                                                                    
 name: my-local-pv       ###referenced by pvc volumeName
spec:                                                                                                                                                                                        
 capacity:                                                                                                                                                                                  
   storage: 5Gi                                                                                                                                                                             
 accessModes:                                                                                                                                                                               
 - ReadWriteOnce                                                                                                                                                                            
 persistentVolumeReclaimPolicy: Retain                                                                                                                                                      
 storageClassName: my-local-storage                                                                                                                                                         
 local:                                                                                                                                                                                     
   path: /mnt/vol1                                                                                                                                                                          
 nodeAffinity:                                                                                                                                                                              
   required:                                                                                                                                                                                
     nodeSelectorTerms:                                                                                                                                                                     
     - matchExpressions:                                                                                                                                                                    
       - key: kubernetes.io/hostname                                                                                                                                                        
         operator: In                                                                                                                                                                       
         values:                                                                                                                                                                            
         - k8s-n1                                                                                                                                                                           

[k8s-m1 ]# kubectl get pv
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS       REASON   AGE
my-local-pv   5Gi        RWO            Retain           Bound    default/my-claim   my-local-storage            18m



[k8s-m1 ]# cat persistentVolumeClaim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: my-claim
spec:
 accessModes:
 - ReadWriteOnce
 storageClassName: my-local-storage
 resources:
   requests:
     storage: 3Gi
 volumeName: my-local-pv                 ###reference pv name
 volumeMode: Filesystem

[k8s-m1 ]# kubectl get pvc
NAME       STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS       AGE
my-claim   Bound    my-local-pv   5Gi        RWO            my-local-storage   17m


[k8s-m1 ]# cat http-pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: www
 labels:
   name: www
spec:
 containers:
 - name: www
   image: dockerhub.abc.com/it/nginx:alpine
   ports:
     - containerPort: 80
       name: www
   volumeMounts:
     - name: www-persistent-storage
       mountPath: /usr/share/nginx/html
 volumes:
   - name: www-persistent-storage
     persistentVolumeClaim:
       claimName: my-claim

PWD:~/study/persistentVolumn  EXIT:0 TIME:2020-0422-16:54:04CST USER:root
[k8s-m1 ]# kubectl get po -o wide
NAME                  READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
www                   1/1     Running   0          4m50s   10.249.1.30   k8s-n1   <none>           <none>

--20
[root@k8s-n1 ~]# mkdir -p /mnt/vol1
[root@k8s-n1 ~]echo "Hello local persistent volume" > /mnt/disk/vol1/index.html

--30
[k8s-m1 ]# curl 10.249.1.30
Hello local persistent volume

#persistentVolumn pv
